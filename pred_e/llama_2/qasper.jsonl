{"pred": "  They looked at different Facebook pages, which they chose based on their performance on the development set and on the distribution of emotions on different pages and in different datasets.", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": "  Unanswerable. The article does not provide information on the type of latent context used to predict instructor intervention.", "answers": ["the series of posts that trigger an intervention"], "all_classes": null, "length": 3732}
{"pred": "  Unanswerable. The article does not provide information on any other evaluation metrics beyond accuracy.", "answers": ["F-score, Kappa", "Unanswerable"], "all_classes": null, "length": 3544}
{"pred": "  The baselines were:\n\n* Plain stacked LSTMs\n* Models with different INLINEFORM0\n* Models without INLINEFORM1\n* Models that integrate lower contexts via peephole connections.", "answers": ["(i) models that use plain stacked LSTMs, (ii) models with different INLINEFORM0, (iii) models without INLINEFORM1, (iv) models that integrate lower contexts via peephole connections"], "all_classes": null, "length": 3224}
{"pred": "  Yes", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2284}
{"pred": "  Yes", "answers": ["No"], "all_classes": null, "length": 3843}
{"pred": "  Yes.", "answers": ["Yes", "No"], "all_classes": null, "length": 2317}
{"pred": "  The datasets used in evaluation are:\n\n* English reviews\n* Spanish restaurant review\n* Turkish restaurant review\n* Dutch restaurant review\n* Russian restaurant review\n\nYes.", "answers": ["SemEval-2016 Challenge Task 5 BIBREF27 , BIBREF28", " English reviews ,  restaurant reviews from four different languages (Spanish, Turkish, Dutch, Russian)"], "all_classes": null, "length": 2720}
{"pred": "  Unanswerable. The article does not provide information on the improvements of small-scale unbalanced datasets when sentence representation is enhanced with topic information.", "answers": ["7.36% on accuracy and 9.69% on F1 score", "it has 0.024 improvement in accuracy comparing to ELMO Only and  0.006 improvement in F1 score  comparing to ELMO Only too"], "all_classes": null, "length": 2890}
{"pred": "  Yes.", "answers": ["They use text transcription.", "both"], "all_classes": null, "length": 3198}
{"pred": "  Yes. According to the article, clustering algorithms used were K-means and hierarchical clustering.", "answers": ["CLUTO, Carrot2 Lingo", "simple clustering algorithm which uses the cosine similarity between word embeddings"], "all_classes": null, "length": 3039}
{"pred": "  Yes. According to the article, the authors used a neural model called BERT to answer questions about privacy policies.", "answers": ["Bert + Unanswerable", "CNN, BERT"], "all_classes": null, "length": 3845}
{"pred": "  The pivot-based baselines are:\n\n* Pivoting: translates source to pivot then to target in two steps, causing inefficient translation process.\n* Multilingual NMT (MNMT): enables training a single model that supports translation from multiple source languages into multiple target languages, even those unseen language pairs.", "answers": ["pivoting, pivoting$_{\\rm m}$", "firstly translates a source language into the pivot language which is later translated to the target language"], "all_classes": null, "length": 3815}
{"pred": "  The article mentions the following datasets were used:\n\n* EmotionLines\n* Friends\n* EmotionPush\n\nSo, the answer is:\n\nYes", "answers": ["Friends, EmotionPush", "EmotionLines BIBREF6"], "all_classes": null, "length": 3178}
{"pred": "  Yes, evaluation protocols are provided.", "answers": ["three experimental setups with different numbers of speakers in the evaluation set, three experimental setups with different number of speaker in the evaluation set are defined,  first one, respondents with at least 17 recording sessions are included to the evaluation set, respondents with 16 sessions to the development and the rest of respondents to the background set, second setup, respondents with at least 8 sessions are included to the evaluation set, respondents with 6 or 7 sessions to the development and the rest of respondents to the background set"], "all_classes": null, "length": 3880}
{"pred": "  Gaussian-masked directional multi-head attention works by dividing queries, keys, and values into different heads and getting information from different subspaces.", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": "  Unanswerable. The article does not provide information on the amount of training data used by the system for non-English languages.", "answers": ["No data. Pretrained model is used."], "all_classes": null, "length": 3409}
{"pred": "  According to the article, attention and alignment differ in the following cases:\n\n* For nouns, attention tends to be concentrated, meaning that the attention distribution is more focused on a single source word.\n* For verbs, attention is more distributed, meaning that the attention distribution is more spread out across multiple source words.\n\nTherefore, the answer to the question is \"unanswerable\" because the article does", "answers": ["For certain POS tags, e.g. VERB, PRON.", "most word alignments only involve one or a few words, attention can be distributed more freely"], "all_classes": null, "length": 3366}
{"pred": "  Yes. According to the article, the authors use a Recurrent Neural Network (RNN) for end-to-end speech recognition.", "answers": ["RNN", " Recurrent Neural Network (RNN)"], "all_classes": null, "length": 1644}
{"pred": "  Yes. According to the article, the baseline for the NER task in Nepali language is a neural network architecture based on BiLSTM and CNN, which uses pre-trained word embeddings and POS tags as input features.", "answers": ["CNN modelBIBREF0, Stanford CRF modelBIBREF21", "Bam et al. SVM, Ma and Hovy w/glove, Lample et al. w/fastText, Lample et al. w/word2vec"], "all_classes": null, "length": 2836}
{"pred": "  The paper was published in 2022, so the answer is \"2022\".", "answers": ["Unanswerable"], "all_classes": null, "length": 3885}
{"pred": "  According to the article, the evaluation study of table-to-text models involved collecting human judgments on the quality of 16 models trained for various tasks. Therefore, the answer to your question is \"unanswerable\" because the article does not provide information on the number of people who participated in the evaluation study.", "answers": ["about 500", "Unanswerable"], "all_classes": null, "length": 3831}
{"pred": "  The models used in the experiment are:\n\n* Linear SVM\n* Bidirectional LSTM\n* Convolutional Neural Network (CNN)", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": "  They explore BiLSTM, BiLSTM-CNN, BiLSTM-CRF, BiLSTM-CNN-CRF, and BiLSTM+CNN(grapheme-level) models.\n\nUnanswerable: Can they initialize the grapheme-level embedding with fasttext embeddings?", "answers": ["BiLSTM, BiLSTM-CNN, BiLSTM-CRF, BiLSTM-CNN-CRF", "BiLSTMBIBREF14, BiLSTM+CNNBIBREF20, BiLSTM+CRFBIBREF1, BiLSTM+CNN+CRFBIBREF2, CNN modelBIBREF0 and Stanford CRF modelBIBREF21"], "all_classes": null, "length": 2838}
{"pred": "  Unanswerable. The article does not provide information on how the vocabulary of word-like or phoneme-like units is automatically discovered.", "answers": ["Zero Resource Toolkit (ZRTools) BIBREF7"], "all_classes": null, "length": 3381}
{"pred": "  Yes. According to the article, the BERT model tested is BERTBase.", "answers": ["BERTbase", "BERTbase"], "all_classes": null, "length": 3852}
{"pred": "  Yes. According to the article, the following keyphrase extraction models were reassessed: \"the first two are commonly used as baselines, the third is a resource-lean unsupervised graph-based ranking approach, and the last two were among the top performing systems in the SemEval-2010 keyphrase extraction task.\"", "answers": ["Answer with content missing: (LVL1, LVL2, LVL3) \n- Stanford CoreNLP\n- Optical Character Recognition (OCR) system, ParsCIT \n- further abridge the input text from level 2 preprocessed documents to the following: title, headers, abstract, introduction, related work, background and conclusion."], "all_classes": null, "length": 1822}
{"pred": "  Yes", "answers": ["Yes", "Yes"], "all_classes": null, "length": 2077}
{"pred": "  The article uses the MIMIC-III dataset, which is a freely available, deidentified database containing electronic health records of patients admitted to an Intensive Care Unit (ICU) at Beth Israel Deaconess Medical Center between 2001 and 2012.", "answers": ["MIMIC-III", "MIMIC-III"], "all_classes": null, "length": 2992}
{"pred": "  According to the article, the length of the dataset for each step of the hierarchy is as follows:\n\n* Level A: Offensive language detection - 14,100 tweets\n* Level B: Categorization of offensive language - 1,075 instances for the GRP class, 2,407 instances for the IND class, and 395 instances for the OTH class\n* Level C: Offensive language target identification - 395 instances\n\nTherefore, the answer to your question is:\n\nLevel A: 14,100 tweets\nLevel B: 3,877 instances\nLevel C: 395 instances\n\nUnanswerable: The article does not provide the length of the dataset for each step of the hierarchy.", "answers": ["Level A: 14100 Tweets\nLevel B: 4640 Tweets\nLevel C: 4089 Tweets"], "all_classes": null, "length": 2255}
{"pred": "  Based on the article, attention captures useful information for translating verbs, including information beyond alignment. The article states that \"the attention of verbs is the closest to alignments on average, but the average attention loss for verbs is almost two times larger than the loss for nouns.\" This suggests that attention captures information beyond alignment, as the attention loss for verbs is higher than for nouns, despite the", "answers": ["it captures other information rather than only the translational equivalent in the case of verbs", "Alignment points of the POS tags."], "all_classes": null, "length": 3372}
{"pred": "  The baselines were:\n\n* Bag-of-words (BOW)\n* Term frequency–inverse document frequency (TFIDF)\n* Neural-based word embedding", "answers": ["BOW-LR, BOW-RF. TFIDF-RF, TextCNN, C-TextCNN", "bag-of-words (BOW), term frequency–inverse document frequency (TFIDF), neural-based word embedding, Logistic Regression (LR), Random Forest (RF), TextCNN BIBREF10 with initial word embedding as GloVe"], "all_classes": null, "length": 3181}
{"pred": "  According to the article, they look at 30% of the Amazon search terms contain parts of e-book titles. Therefore, the answer is \"unanswerable\" as the article does not provide the exact number of tags they look at.", "answers": ["Unanswerable", "48,705"], "all_classes": null, "length": 3307}
{"pred": "  Yes, the model architecture is provided in the article. According to the article, the model uses a Transformer base architecture with 3 encoder layers, 3 decoder layers, and 0.3 dropout.", "answers": ["follow the architecture in berard2018end, but have 3 decoder layers like that in pino2019harnessing"], "all_classes": null, "length": 2424}
{"pred": "  They used the Penn Treebank and WikiText2 datasets.\n\nYes.", "answers": [" Penn Treebank, WikiText2", "Penn Treebank (PTB) , WikiText2 (WT-2)"], "all_classes": null, "length": 3302}
{"pred": "  Yes.", "answers": ["No", "No"], "all_classes": null, "length": 3158}
{"pred": "  Unanswerable. The article does not provide information on why prior knowledge distillation techniques are ineffective in producing student models with vocabularies different from the original teacher models.", "answers": ["While there has been existing work on reducing NLP model vocabulary sizes BIBREF15, distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes.", "distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes."], "all_classes": null, "length": 3570}
{"pred": "  Yes. According to the article, the baseline method used is the word2vec algorithm.", "answers": ["using word2vec to create features that are used as input to the SVM", "use the word2vec algorithm, create several unsupervised hand-crafted features, generate document vectors and feed them as input into the support vector machines (SVM) approach"], "all_classes": null, "length": 3820}
{"pred": "  The ancient Chinese dataset comes from a large-scale parallel corpus which contains INLINEFORM0 1.24M bilingual sentence pairs.", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": "  Yes.", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": "  Based on the article, the more predictive set of features to detect fake news is:\n\n* Words embeddings\n* Style\n* Morality\n\nYes.", "answers": ["words embeddings, style, and morality features", "words embeddings, style, and morality features"], "all_classes": null, "length": 2091}
{"pred": "  The article does not provide information on the datasets used for evaluation. Therefore, the answer is \"unanswerable\".", "answers": ["United Nations Parallel Corpus, IIT Bombay corpus, OpenSubtitles 2018"], "all_classes": null, "length": 3405}
{"pred": "  Based on the article, the answer to the question is:\n\nUnanswerable.\n\nThe article does not provide any information about the languages for which word embeddings are built.", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 2009}
{"pred": "  Yes.", "answers": ["No", "No"], "all_classes": null, "length": 1899}
{"pred": "  The collection of COVID-19 literature is large, with over 45,000 scholarly articles included in the CORD-19 dataset.", "answers": ["45,000 scholarly articles, including over 33,000 with full text"], "all_classes": null, "length": 2150}
{"pred": "  The proposed model is compared to the following baseline models:\n\n1. Traditional features (TF-IDF)\n2. Deep features (CNNs, LSTMs)\n3. ELMo (a new type of word representation)\n\nTherefore, the answer is \"yes\".", "answers": ["support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self", "support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self"], "all_classes": null, "length": 2895}
{"pred": "  Unanswerable", "answers": ["1913 signals", "Unanswerable"], "all_classes": null, "length": 2361}
{"pred": "  The different modules in Macaw are:\n\n* Co-Reference Resolution\n* Query Generation\n* Retrieval Model\n* Result Generation\n\nYes.", "answers": ["Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation", "Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation"], "all_classes": null, "length": 1701}
{"pred": "  Yes.", "answers": ["Unanswerable"], "all_classes": null, "length": 1913}
{"pred": "  Unanswerable. The article does not provide information about the accents present in the corpus.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": "  According to the article, experiments are performed on the SQuAD dataset.", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": "  According to the article, the authors train their models on the SemEval 2010 task 8 dataset. Therefore, the answer is:\n\nYes", "answers": ["relation classification dataset of the SemEval 2010 task 8", "SemEval 2010 task 8 BIBREF8"], "all_classes": null, "length": 2393}
{"pred": "  The article does not provide information on the size of the corpora used to train ELMo. Therefore, the answer is \"unanswerable\".", "answers": ["2174000000, 989000000", "2174 million tokens for English and 989 million tokens for Russian"], "all_classes": null, "length": 2958}
{"pred": "  The qualitative experiments performed on benchmark datasets are listed in Table TABREF9.", "answers": ["Spearman correlation values of GM_KL model evaluated on the benchmark word similarity datasets.\nEvaluation results of GM_KL model on the entailment datasets such as entailment pairs dataset created from WordNet, crowdsourced dataset of 79 semantic relations labelled as entailed or not and annotated distributionally similar nouns dataset.", "Given a query word and component id, the set of nearest neighbours along with their respective component ids are listed"], "all_classes": null, "length": 2220}
{"pred": "  According to the article, the method improvements of F1 for paraphrase identification are:\n\n* For BERT, F1 score improves by +0.58 for MRPC and +0.73 for QQP when using Dice loss (DL) instead of cross-entropy loss.\n* For XLNet, F1 score improves by +0.73 for MRPC and +", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": "  Unanswerable. The article does not provide any information on how to calculate variance from the model outputs.", "answers": ["reducing the variance of an estimator,  EGL method in BIBREF3 is almost the same as Eq. ( EQREF8 ), except the gradient's norm is not squared in BIBREF3", " Fisher Information Ratio"], "all_classes": null, "length": 1671}
{"pred": "  The encoder has an LSTM architecture.", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "  The questions present in the dataset are natural language questions over the 500 most popular articles of Wikipedia.", "answers": ["These 8 tasks require different competencies and a different level of understanding of the document to be well answered"], "all_classes": null, "length": 3817}
{"pred": "  Yes", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": "  The article states that the corpus consists of 53 documents. Therefore, the answer is: 53 documents.", "answers": ["53 documents", "53 documents"], "all_classes": null, "length": 2667}
{"pred": "  Yes. According to the article, the recipes are scraped from Food.com, a popular recipe sharing website.", "answers": ["from Food.com"], "all_classes": null, "length": 2649}
{"pred": "  Based on the article, they evaluate their resulting word embeddings using intrinsic evaluation tasks such as word similarity and word analogy tasks, as well as downstream tasks. They also verify the ability to represent OOV words by quantitatively evaluating nearest-neighbors. Therefore, the answer is \"yes\".", "answers": ["We also evaluate all five models on downstream tasks from the VecEval suite BIBREF13 , using only the tasks for which training and evaluation data is freely available: chunking, sentiment and question classification, and natural language identification (NLI). The default settings from the suite are used, but we run only the fixed settings, where the embeddings themselves are not tunable parameters of the models, forcing the system to use only the information already in the embeddings."], "all_classes": null, "length": 2083}
{"pred": "  Unanswerable. The article does not provide information on 10 other phenotypes that are annotated.", "answers": ["Adv. Heart Disease, Adv. Lung Disease, Alcohol Abuse, Chronic Neurologic Dystrophies, Dementia, Depression, Developmental Delay, Obesity, Psychiatric disorders and Substance Abuse"], "all_classes": null, "length": 1651}
{"pred": "  The average length of the essays in the dataset is not provided in the article, so the answer is \"unanswerable\".", "answers": ["204 tokens", "Most texts, however, range roughly from 150 to 250 tokens."], "all_classes": null, "length": 1898}
{"pred": "  Yes, the article mentions that textual patterns are extracted for generating errors.", "answers": ["(VVD shop_VV0 II, VVD shopping_VVG II)", "patterns for generating all types of errors"], "all_classes": null, "length": 2133}
{"pred": "  Yes", "answers": ["No", "No"], "all_classes": null, "length": 3974}
{"pred": "  The Random Kitchen Sink (RKS) approach is a method for explicit mapping of data vectors to a space where linear separation is possible, as proposed in the article.", "answers": ["Random Kitchen Sink method uses a kernel function to map data vectors to a space where linear separation is possible.", "explicitly maps data vectors to a space where linear separation is possible, RKS method provides an approximate kernel function via explicit mapping"], "all_classes": null, "length": 2361}
{"pred": "  They compare to BIBREF26 who use a task-specific architecture compared to their generic sequence to sequence baseline.", "answers": ["BIBREF11 , BIBREF26 "], "all_classes": null, "length": 1912}
{"pred": "  Yes. According to the article, the results of using artificial data for error detection are shown in Table TABREF1 and Table TABREF4, which contain the performance of the error detection systems on the FCE test data and the two alternative annotations of the CoNLL 2014 Shared Task dataset, respectively.", "answers": ["Combining  pattern based and Machine translation approaches gave the best overall F0.5 scores. It was 49.11 for FCE dataset  , 21.87 for the first annotation of  CoNLL-14, and 30.13 for the second annotation of CoNLL-14. "], "all_classes": null, "length": 2164}
{"pred": "  They test their method on hashtag prediction for social media.", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": "  Yes. According to the article, the experts used for annotation were seven experts with legal training.", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": "  Based on the article, the following approaches have been applied to solve word segmentation in Vietnamese:\n\n* Machine learning-based methods (e.g., Support Vector Machine, Conditional Random Fields)\n* Lexical analysis, syntactic analysis, semantic analysis, and disclosure analysis\n* Word segmentation using B-I-O tagset\n* Using context around a word segment position\n* Using additional features", "answers": ["Maximum Entropy, Weighted Finite State Transducer (WFST),  support vector machines (SVM), conditional random fields (CRF)", "Maximum matching, Hidden Markov model , Maximum Entropy, Conditional Random Fields , Support Vector Machines"], "all_classes": null, "length": 3471}
{"pred": "  Yes. According to the article, the following named entity recognition models were evaluated:\n\n1. Stanford NER\n2. spaCy 2.0\n3. a recurrent model similar to BIBREF13 and BIBREF14\n\nThese models were evaluated using the manually annotated gold dataset.", "answers": ["Stanford NER, spaCy 2.0 , recurrent model with a CRF top layer", "Stanford NER, spaCy 2.0, recurrent model with a CRF top layer"], "all_classes": null, "length": 2759}
{"pred": "  According to the article, the datasets used to evaluate the paper are:\n\n* WN18\n* FB15K", "answers": ["WordNet BIBREF0, Freebase BIBREF1, WN18 (a subset of WordNet) BIBREF24 , FB15K (a subset of Freebase) BIBREF2"], "all_classes": null, "length": 3367}
{"pred": "  Yes. According to the article, the baseline model used is \"LastStateRNN\".", "answers": [" LastStateRNN, AvgRNN, AttentionRNN", "LastStateRNN, AvgRNN, AttentionRNN "], "all_classes": null, "length": 2823}
{"pred": "  They addressed three topics of cyberbullying: personal attack, racism, and sexism.", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": "  Unanswerable. The article does not provide information on the language or location of the data used in the analysis.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3634}
{"pred": "  The dataset used for this study is an existing, annotated Twitter dataset that was constructed based on a hierarchical model of depression-related symptoms.", "answers": ["BIBREF12 , BIBREF13", "an existing, annotated Twitter dataset that was constructed based on a hierarchical model of depression-related symptoms BIBREF12 , BIBREF13"], "all_classes": null, "length": 1939}
{"pred": "  Yes.", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": "  The sentiment analysis dataset used is IMDb dataset.\n\nUnanswerable: Can you provide more information about the project funding?", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": "  Yes. According to the article, the system's performance was evaluated using a dataset of 14.5K tweets, and the results showed that the system was able to identify offensive speech with a high degree of accuracy.", "answers": ["accuracy and F1-score of 89.6% and 89.2%, respectively", "accuracy and F1-score of 89.6% and 89.2%, respectively"], "all_classes": null, "length": 3313}
{"pred": "  Yes. According to the article, AEM outperforms K-means, LEM, and DPEMM on the three datasets.", "answers": ["K-means, LEM BIBREF13, DPEMM BIBREF14", "K-means, LEM, DPEMM"], "all_classes": null, "length": 3841}
{"pred": "  Yes. According to the article, PolyResponse engine is used for restaurant search and booking in 8 languages: English, German, Spanish, Mandarin, Polish, Russian, and Korean.", "answers": ["English, German, Spanish, Mandarin, Polish, Russian, Korean and Serbian", "English (Edinburgh), German (Berlin), Spanish (Madrid), Mandarin (Taipei), Polish (Warsaw), Russian (Moscow), Korean (Seoul), and Serbian (Belgrade)"], "all_classes": null, "length": 2754}
{"pred": "  The answer to your question is \"unanswerable\" based on the information provided in the article. The article does not mention the sources of the datasets used for the experiments.", "answers": ["rupnik2016news", "rupnik2016news, Deutsche Welle's news website"], "all_classes": null, "length": 3160}
{"pred": "  Unanswerable.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1868}
{"pred": "  Other sentence embeddings methods evaluated in the article are:\n\n* InferSent\n* Universal Sentence Encoder\n* XLNet\n* RoBERTa\n* Skip-Thought\n* Conneau et al.\n* polyencoders\n\nNote: The article does not evaluate all of these methods directly, but it discusses their performance on various benchmark tasks and compares them to", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": "  Yes, the article evaluates the following transfer learning tasks:\n\n1. Semantic similarity comparison\n2. Clustering\n3. Information retrieval via semantic search\n4. Question-type classification from TREC\n5. Subjectivity prediction of sentences from movie reviews and plot summaries\n6. Phrase level opinion polarity classification from newswire\n7. Stanford Sentiment Treebank with", "answers": ["MR, CR, SUBJ, MPQA, SST, TREC, MRPC", "MR: Sentiment prediction for movie reviews snippets on a five start scale BIBREF25.\n\nCR: Sentiment prediction of customer product reviews BIBREF26.\n\nSUBJ: Subjectivity prediction of sentences from movie reviews and plot summaries BIBREF27.\n\nMPQA: Phrase level opinion polarity classification from newswire BIBREF28.\n\nSST: Stanford Sentiment Treebank with binary labels BIBREF29.\n\nTREC: Fine grained question-type classification from TREC BIBREF30.\n\nMRPC: Microsoft Research Paraphrase Corpus from parallel news sources BIBREF31.", "Semantic Textual Similarity, sentiment prediction, subjectivity prediction, phrase level opinion polarity classification, Stanford Sentiment Treebank, fine grained question-type classification."], "all_classes": null, "length": 3861}
{"pred": "  The vocabulary size is unanswerable based on the information provided in the article.", "answers": ["33,663", "33,663 distinct review keywords "], "all_classes": null, "length": 3305}
{"pred": "  Proposed additional features and context:\n\n* Word-level features using n-grams ranging from 1 to 3\n* Character-level features from 3 to 8-grams\n* GloVe representation for word-level features\n* Context tweets as additional feature for neural network models\n* LTC (Latent Topic Clustering) method for neural network models\n\nUnanswerable: Can the article answer the question? No.", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": "  Yes", "answers": ["No", "No"], "all_classes": null, "length": 1906}
{"pred": "  Yes.", "answers": ["No", "No"], "all_classes": null, "length": 1443}
{"pred": "  Yes. According to the article, the best performing model among the author's submissions is the ensemble+ of (r4, r7, r12) on the test set, with a F1 score of 0.673.", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": "  The corpus used for the task is \"DTA18 and DTA19\" (unanswerable).", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": "  The size of the dataset is 10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs.", "answers": ["13,757", "10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs"], "all_classes": null, "length": 3704}
{"pred": "  Yes. According to the article, the following classifiers have been trained: K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron (MLP).", "answers": ["KNN\nRF\nSVM\nMLP", " K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), Multi-layer Perceptron (MLP)"], "all_classes": null, "length": 1639}
{"pred": "  They propose to use two contexts: (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context.", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": "  Multi-granularity and multi-tasking are specific to the neural architecture design in the article.", "answers": ["An output layer for each task", "Multi-tasking is addressed by neural sequence tagger based on LSTM-CRF and linguistic features, while multi-granularity is addressed by ensemble of LSTM-CRF and BERT."], "all_classes": null, "length": 1514}
{"pred": "  The CORD-19 dataset is a collection of scholarly articles about COVID-19, SARS-CoV-2, and related coronaviruses, contributed by hospitals and medical institutes all over the world.", "answers": ["which contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses", "contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses"], "all_classes": null, "length": 2156}
{"pred": "  Unanswerable. The article does not provide information on the size of the real-life dataset used in the study.", "answers": ["26972", "26972 sentences"], "all_classes": null, "length": 3040}
{"pred": "  Based on the article, the state-of-the-art methods for grammar induction are:\n\n1. Compound Probabilistic Context-Free Grammars (compound PCFGs): This approach models rich generative processes by marginalizing over the latent parameter, allowing for tractable training.\n2. Neural PCFGs: This method uses a neural network to parameterize rule probabilities", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2533}
{"pred": "  The backoff strategies work by providing a modular approach to tackle character-level adversarial attacks. The word recognition model ( $W$ ) is placed before the downstream classifier ( $C$ ), and the combined model's accuracy is governed by two factors: $W$ 's accuracy in recognizing misspelled words and $W$ 's sensitivity to adversarial perturbations on the same input", "answers": ["In pass-through, the recognizer passes on the possibly misspelled word, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.", "Pass-through passes the possibly misspelled word as is, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.", "Backoff to \"a\" when an UNK-predicted word is encountered, backoff to a more generic word recognition model when the model predicts UNK"], "all_classes": null, "length": 4210}
{"pred": "  The Neural User Simulator (NUS) uses a corpus of recorded dialogues to learn behavior.\n\nUnanswerable: Can the NUS generate its own goal and change it during a dialogue?", "answers": ["DSTC2", "The manual transcriptions of the DSTC2 training set "], "all_classes": null, "length": 4820}
{"pred": "  According to the article, the following metrics are used for evaluation:\n\n* BLEU-4\n* NIST-4\n* ROUGE-4\n\nTherefore, the answer is \"yes\".", "answers": ["BLEU-4, NIST-4, ROUGE-4", "BLEU-4, NIST-4, ROUGE-4"], "all_classes": null, "length": 4460}
{"pred": "  Based on the article, the state-of-the-art models for judgment prediction in civil cases are:\n\n* Legal Reading Comprehension (LRC)\n* AutoJudge\n\nThese models are proposed in the article and are based on the idea of incorporating law articles into the judgment prediction process. The article does not provide information on other state-of-the-art models, so the", "answers": ["SVM , CNN , GRU , CNN/GRU+law, r-net , AoA ", "SVM with lexical features in accordance with previous works BIBREF16 , BIBREF17 , BIBREF1 , BIBREF15 , BIBREF4, attention-based method BIBREF3 and other methods we deem important, some off-the-shelf RC models, including r-net BIBREF5 and AoA BIBREF6 , which are the leading models on SQuAD leaderboard"], "all_classes": null, "length": 4106}
{"pred": "  The article mentions the following linguistics features are used for Arabic diacritics recovery:\n\n* Word surface forms\n* POS information, including gender and number of stems, prefixes, and suffixes\n* Morphological patterns\n* Word bigram to constrain the PRIOR feature\n* Named entities\n\nTherefore, the answer is:\n\nYes", "answers": ["POS, gender/number and stem POS"], "all_classes": null, "length": 4719}
{"pred": "  Unanswerable. The article does not provide information on which dimension semantically related words take larger values.", "answers": ["dimension corresponding to the concept that the particular word belongs to"], "all_classes": null, "length": 6182}
{"pred": "  English", "answers": ["English , German, French"], "all_classes": null, "length": 4173}
{"pred": "  Unanswerable. The article does not provide information on whether their framework automatically optimizes for hyperparameters.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 4465}
{"pred": "  Yes. According to the article, the two news domains that are country-independent are disinformation and mainstream news.", "answers": ["mainstream news and disinformation", "mainstream and disinformation news"], "all_classes": null, "length": 4856}
{"pred": "  The experiments are performed on text-based games called CoinCollector and CookingWorld.", "answers": ["CoinCollector , CookingWorld ", "CoinCollector, CookingWorld"], "all_classes": null, "length": 4924}
{"pred": "  The two metrics proposed in the article are:\n\n1. F INLINEFORM0 @ INLINEFORM1\n2. F INLINEFORM2 @ INLINEFORM3", "answers": ["average unique predictions, randomly sample 2000 decoder hidden states at INLINEFORM4 steps following a delimiter ( INLINEFORM5 ) and apply an unsupervised clustering method (t-SNE BIBREF35 )"], "all_classes": null, "length": 4581}
{"pred": "  Unanswerable. The article does not provide information on the biases captured by the model.", "answers": ["Data annotation biases where tweet containing disrespectful words are annotated as hate or offensive without any presumption about the social context of tweeters"], "all_classes": null, "length": 4111}
{"pred": "  Unanswerable. The article does not provide any information about future possible improvements for the proposed hierarchical encoder for structured data.", "answers": ["rther constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions", "to further constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions"], "all_classes": null, "length": 4704}
{"pred": "  The additive modification to the objective function is \"introducing a modification to the embedding learning process by introducing a cost term that favors an increase for the value of a word's embedding vector dimension corresponding to the concept that the particular word belongs to\".", "answers": ["The cost function for any one of the words of concept word-groups is modified by the introduction of an additive term to the cost function. . Each embedding vector dimension is first associated with a concept. For a word belonging to any one of the word-groups representing these concepts, the modified cost term favors an increase for the value of this word's embedding vector dimension corresponding to the concept that the particular word belongs to,", "An additive term added to the cost function for any one of the words of concept word-groups"], "all_classes": null, "length": 6244}
{"pred": "  Unanswerable. The article does not provide information on the scope of the results, including whether they are limited to English data or not.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 7837}
{"pred": "  The novelty of their document-level encoder is mentioned in the article as follows: \"We propose a novel document-level encoder based on Bert which encodes a document and obtain representations for its sentences.\" Therefore, the answer to your question is \"novel\".", "answers": ["Bert model have a maximum length of 512; we overcome this limitation by adding more position embeddings, we insert external [cls] tokens at the start of each sentence, and each [cls] symbol collects features for the sentence preceding it, document representations are learned hierarchically"], "all_classes": null, "length": 4404}
{"pred": "  The authors claim that their model improves interpretability compared to softmax transformers by using attention mechanisms that learn to look for various relationships between words, rather than just focusing on the overall relevance of each word. They also propose a new attention distribution, which is computed using a softmax normalizing transform, but with a twist: the attention weights are not necessarily continuous, but can take on discrete values. This", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": "  Unanswerable. The article does not provide information on how the use of morphological segmentation improves the efficacy of the attention mechanism in machine translation.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 5331}
{"pred": "  Unanswerable", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": "  Yes.", "answers": ["sentence"], "all_classes": null, "length": 4369}
{"pred": "  According to the article, two new datasets are developed and released publicly for the task of forecasting derailment of online conversations. One dataset is entirely new, while the other relies on in-the-wild moderation of the popular subreddit ChangeMyView. The article does not provide information on the labels available in these datasets for antisocial events, so the answer is \"unanswerable\".", "answers": ["The Conversations Gone Awry dataset is labelled as either containing a personal attack from withint (i.e. hostile behavior by one user in the conversation directed towards another) or remaining civil throughout. The Reddit Change My View dataset is labelled with whether or not a coversation eventually had a comment removed by a moderator for violation of Rule 2: \"Don't be rude or hostile to others users.\""], "all_classes": null, "length": 4779}
{"pred": "  According to the article, the datasets used in the experiment are:\n\n* INLINEFORM0 case documents that the Supreme People's Court of People's Republic of China has made publicly available.\n\nTherefore, the answer is: INLINEFORM0.", "answers": ["build a new one, collect INLINEFORM0 cases from China Judgments Online"], "all_classes": null, "length": 4109}
{"pred": "  Based on the article, they measure style transfer success by evaluating the model's ability to transfer style in the following ways:\n\n* By checking the output of the model on a test set of sentences and assessing its ability to match the desired style.\n* By analyzing the proportions of sentence pairs in the test set where there was a significant difference between translations into different styles.\n* By using human evaluators", "answers": ["Unanswerable"], "all_classes": null, "length": 4572}
{"pred": "  Yes. According to the article, the proposed method achieves 65% accuracy in predicting sensationalism, which is an improvement over the best performing state-of-the-art method, which achieves 42.6% sensationalism score.", "answers": ["absolute improvement of 18.2% over the Pointer-Gen baseline"], "all_classes": null, "length": 4091}
{"pred": "  Based on the article, the answer to the question \"How were the human judgements assembled?\" is \"unanswerable\" because the article does not provide any information on how the human judgements were assembled.", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": "  Yes, they use a joint model combining textual and visual representations to predict the quality of a document.", "answers": ["visual model is based on fine-tuning an Inception V3 model BIBREF1 over visual renderings of documents, while our textual model is based on a hierarchical biLSTM. We further combine the two into a joint model. , neural network models", "Inception V3, biLSTM"], "all_classes": null, "length": 4217}
{"pred": "  Yes. According to the article, the baseline for the translation task was a strong M2M Transformer model trained on the mixture of six-way pseudo-parallel data generated by VII and the original parallel data.", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": "  The global network features which quantify different aspects of the sharing process are:\n\n* LWCC (size of the largest cascade in the layer)\n* SCC (size of the network)\n* DWCC (depth of the largest cascade in the layer)\n\nYes.", "answers": ["Number of Strongly Connected Components (SCC), Size of the Largest Strongly Connected Component (LSCC), Number of Weakly Connected Components (WCC), Size of the Largest Weakly Connected Component (LWCC), Diameter of the Largest Weakly Connected Component (DWCC), Average Clustering Coefficient (CC), Main K-core Number (KC), Density (d)"], "all_classes": null, "length": 4906}
{"pred": "  Yes. According to the article, the baseline is the performance of the models on the human written transcripts with human (defined) segmentation.", "answers": ["QA PGNet, Multi-decoder QA PGNet with lookup table embedding", "QA PGNet and Multi-decoder QA PGNet"], "all_classes": null, "length": 4600}
{"pred": "  The core component for KBQA is hierarchical matching between questions and KB relations.", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": "  Yes. The article adapts an off-the-shelf reward learning algorithm from reinforcement learning for joint data manipulation learning and model training.", "answers": ["BIBREF7", " reward learning algorithm BIBREF7"], "all_classes": null, "length": 4841}
{"pred": "  Yes. According to the article, the dataset used for the experiments is the BioASQ dataset, which is a biomedical document classification, document retrieval, and question answering competition.", "answers": ["BioASQ  dataset", "A dataset provided by BioASQ consisting of questions, gold standard documents, snippets, concepts  and ideal and ideal answers."], "all_classes": null, "length": 6810}
{"pred": "  The manual Pyramid scores used are INLINEFORM0 = 0.454.", "answers": [" higher tiers of the pyramid", "following the pyramid framework, we design an annotation scheme"], "all_classes": null, "length": 4771}
{"pred": "  The article does not provide information on how their models decide how much importance to give to the output words. Therefore, the answer is \"unanswerable\".", "answers": ["Given the contribution matrix, we can obtain the word importance of each input word to the entire output sentence. ", "They compute the gradient of the output at each time step with respect to the input words to decide the importance."], "all_classes": null, "length": 4253}
{"pred": "  Yes. According to the article, the state-of-the-art models for sarcasm detection include BIBREF6, BIBREF9, BIBREF10, BIBREF11, BIBREF12, BIBREF13, BIBREF14, BIBREF15, BIBREF16, BIBREF17, BIBREF18, BIB", "answers": ["BIBREF9 , BIBREF8 ", "BIBREF9 , BIBREF8"], "all_classes": null, "length": 4855}
{"pred": "  The article states that the data manipulation approach significantly improves over strong base models on low-data regimes and class-imbalance problems, but does not provide specific improvement numbers. Therefore, the answer is \"unanswerable\".", "answers": ["Low data: SST-5, TREC, IMDB around 1-2 accuracy points better than baseline\nImbalanced labels: the improvement over the base model increases as the data gets more imbalanced, ranging from around 6 accuracy points on 100:1000 to over 20 accuracy points on 20:1000"], "all_classes": null, "length": 4880}
{"pred": "  Yes.", "answers": ["MULTIPLE CHOICE QUESTION ANSWERING", "multiple-choice"], "all_classes": null, "length": 6396}
{"pred": "  The article mentions several publicly available datasets that are used for hate speech detection, including:\n\n* Twitter dataset collected by Waseem et al. (2016)\n* Twitter dataset collected by Davidson et al. (2019)\n* Hatebase lexicon dataset (2019)\n\nTherefore, the answer to your question is:\n\nYes, the article mentions several", "answers": ["Waseem-dataset, Davidson-dataset,", "Waseem and Hovey BIBREF5, Davidson et al. BIBREF9"], "all_classes": null, "length": 4090}
{"pred": "  They test their word importance approach on two representative model architectures: Transformer and a black-box method, which is a neural machine translation (NMT) model.", "answers": [" Transformer BIBREF1 model and the conventional RNN-Search model BIBREF0", "Transformer, RNN-Search model"], "all_classes": null, "length": 4240}
{"pred": "  The article does not provide a clear answer to this question. However, it mentions that the proposed model uses a set generation problem, where the goal is to automatically predict keyphrases that are both multi-word units and summarize the high-level meaning of a source text. The model is also designed to generate multiple keyphrases, and the authors propose new metrics to evaluate keyphrase generation. However, the article does not provide", "answers": ["average unique predictions, illustrate the difference of predictions between our proposed models, we show an example chosen from the KP20k validation set"], "all_classes": null, "length": 4576}
{"pred": "  Yes, the article proposes a novel task called concept-map-based MDS, which is defined as creating a concept map that represents a document cluster's most important content, is connected, and satisfies a specified size limit. The task is complex and involves several interdependent subtasks, including extracting appropriate labels for concepts and relations and recognizing different expressions that refer to the same concept across multiple documents. The article also presents a", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": "  Yes. According to the article, they compared the performance of layer-wise trained LSTM models with Xavier initialization and 6-layers sMBR model, and also compared the performance of 2-layers distilled model with sMBR.", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": "  Yes.", "answers": ["Yes"], "all_classes": null, "length": 4073}
{"pred": "  Yes, KAR is an end-to-end MRC model.", "answers": ["Lexicon Embedding Layer, Context Embedding Layer, Coarse Memory Layer, Refined Memory Layer, Answer Span Prediction Layer"], "all_classes": null, "length": 4133}
{"pred": "  Future work planned: Unanswerable", "answers": ["ethical questions about generating sensational headlines, which can be further explored,  improving the sensationalism scorer, investigating the applications of dynamic balancing methods between RL and MLE"], "all_classes": null, "length": 4104}
{"pred": "  Based on the article, the answer to the question is \"unanswerable\" because the article does not provide information on how the dataset models a character's profile.", "answers": ["attributes are determined by human viewers and their impressions of the characters, and are correlated with human-like characteristics"], "all_classes": null, "length": 5136}
{"pred": "  Reuters database.", "answers": ["Reuters-8 dataset without stop words", "The Reuters-8 dataset (with stop words removed)"], "all_classes": null, "length": 5147}
{"pred": "  The accuracy merits of the approach are demonstrated through experiments and results presented in the article. The approach is shown to significantly outperform the state of the art on real-world datasets, with an average improvement of 24.3% AUC.", "answers": ["significant improvements clearly demonstrate that our approach is effective at improving model performance", "By evaluating the performance of the approach using accuracy and AUC"], "all_classes": null, "length": 4489}
{"pred": "  The authors present evidence that the model can capture some biases in data annotation and collection by examining the results of misclassified items in the test datasets. They provide examples of misclassified items and suggest that the biases in the data collection and annotation process may be responsible for some of the misclassifications. Therefore, the answer is \"yes\".", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4119}
{"pred": "  According to the article, the proposed KBQA system achieves state-of-the-art results on two benchmarks:\n\n* SimpleQuestions: The system achieves a score of 93.3% on this benchmark.\n* WebQuestions: The system achieves a score of 82.53% on this benchmark.\n\nTherefore, the answer to the question is \"yes", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4524}
{"pred": "  Based on the information provided in the article, the model is more reliable for grammatical error correction.", "answers": ["grammatical, spelling and word order errors", "spelling, word order and grammatical errors"], "all_classes": null, "length": 4579}
{"pred": "  Based on the article, the authors found ironic data on Twitter by crawling over 2 million tweets and removing non-ironic tweets using a combination of hand-crafted features and machine learning algorithms.", "answers": ["They developed a classifier to find ironic sentences in twitter data", "by crawling"], "all_classes": null, "length": 4599}
{"pred": "  They mean that the model integrates recent findings on neural extractive summarization in a parameter-lean and modular architecture.", "answers": ["global (the whole document), local context (e.g., the section/topic)", "global (the whole document) and the local context (e.g., the section/topic) "], "all_classes": null, "length": 4287}
{"pred": "  Yes, KBQA abbreviates Knowledge Base Question Answering.", "answers": ["Knowledge Base Question Answering", "Knowledge Base Question Answering "], "all_classes": null, "length": 4521}
{"pred": "  They utilize LDA and Gibbs sampling to evaluate ISWC and WWW publications by applying the algorithm on the conference's publications from 2013-2017.", "answers": ["the LDA approaches to recommendation systems and given the importance of research, we have studied recent impressive articles on this subject and presented a taxonomy of recommendation systems based on LDA of the recent research, we evaluated ISWC and WWW conferences articles from DBLP website and used the Gibbs sampling algorithm as an evaluation parameter", "discover the trends of the topics and find relationship between LDA topics and paper features and generate trust tags,  learn a LDA model with 100 topics; $\\alpha =0.01$, $\\beta = 0.01$ and using Gibbs sampling as a parameter estimation"], "all_classes": null, "length": 4322}
{"pred": "  Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4127}
{"pred": "  The imbalance in the analyzed corpora is significant, with a proportion of 65% men and 35% women.", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4055}
{"pred": "  Yes. According to the article, the baseline model used in the experiments is \"BioBERT\" which is a pre-trained contextualized word embedding model based on BERT.", "answers": ["by answering always YES (in batch 2 and 3) "], "all_classes": null, "length": 6817}
{"pred": "  Yes, their highest recall score was 0.6103 in one of the test batches for Factoid Question Answering task.", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 6810}
{"pred": "  The evaluation criteria and metrics used to evaluate the generated text were:\n\n* Human evaluation: minimum edit evaluation, factual correctness, and fluency\n* Word error rate (WER) measured against the reference\n\nUnanswerable: Can you provide more information about the preliminary experiments you conducted to generate whole news texts at once?", "answers": ["BLEU , NIST , METEOR , ROUGE-L, CIDEr , evaluation script, automatic evaluation, human evaluation, minimum edit evaluation, word error rate (WER), factual errors and their types, fluency issues, acceptability of the output for production use in a news agency", "BLEU, NIST, METEOR, ROUGE-L, CIDEr"], "all_classes": null, "length": 4738}
{"pred": "  Yes. According to the article, the authors used a content-based classifier in conjunction with two feature selection methods for predicting a social media user's industry.", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4177}
{"pred": "  The article uses the following real-world datasets:\n\n* CyberAttack\n* PoliticianDeath\n\nTherefore, the answer is \"yes\".", "answers": ["Tweets related to CyberAttack and tweets related to PoliticianDeath", "cyber security (CyberAttack), death of politicians (PoliticianDeath)"], "all_classes": null, "length": 4481}
{"pred": "  The article mentions that two machine learning (ML) and deep learning (DL) methods are used for RQE:\n\n1. Logistic Regression\n2. Recurrent Neural Networks (RNNs)\n\nTherefore, the answer is:\n\nYes, logistic regression and RNNs are used for RQE.", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 7257}
{"pred": "  Yes. According to the article, the strong baseline is the Seq2Seq model with attention and conditional copy.", "answers": ["Conditional Copy (CC) model ", "delayed copy model (DEL),  template system (TEM), conditional copy (CC), NCP+CC (NCP)"], "all_classes": null, "length": 4746}
{"pred": "  Yes, the invertibility condition is discussed in the article. According to the article, the invertibility condition is a necessary constraint for the neural projector to tackle the train the models on sentences of length INLINEFORM0 , \"head-percolation\" rules BIBREF39 are applied to obtain gold dependencies for evaluation.", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4323}
{"pred": "  According to the article, the target model used for classifying microposts is a deep neural network. Therefore, the answer to the question is \"deep neural network\".", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4475}
{"pred": "  The size of the real-world civil case dataset is INLINEFORM0.", "answers": ["100 000 documents", " INLINEFORM1 cases"], "all_classes": null, "length": 4104}
{"pred": "  The data was collected through crowdsourcing on Amazon Mechanical Turk.", "answers": ["The data was collected using 3 components: describe a series of pilot studies that were conducted to collect commonsense inference questions, then discuss the resulting data collection of questions, texts and answers via crowdsourcing on Amazon Mechanical Turk and gives information about some necessary postprocessing steps and the dataset validation."], "all_classes": null, "length": 4536}
{"pred": "  The article mentions the following methods to reduce data sparsity effects:\n\n* Using subword units instead of word units in the neural machine translation (NMT) system\n* Applying data augmentation methods, such as back-translation and mix-source, to increase the amount of training data\n* Using VNBPE (Vietnamese unsupervised BPE) for Vietnamese word segmentation in the NMT", "answers": ["Back Translation, Mix-Source Approach", "data augmentation"], "all_classes": null, "length": 4123}
{"pred": "  The difference in performance between the proposed model and baselines is significant, with the proposed model achieving a noticeable improvement in Hits@n/N accuracy and other metrics for retrieving the correct response of five diverse characters. (Yes)", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 5151}
{"pred": "  Unanswerable. The article does not provide information on the distinctive characteristics of how Arabic speakers use offensive language.", "answers": ["Frequent use of direct animal name calling, using simile and metaphors, through indirect speech like sarcasm, wishing evil to others, name alteration, societal stratification, immoral behavior and sexually related uses.", "Direct name calling, Simile and metaphor, Indirect speech, Wishing Evil, Name alteration, Societal stratification, Immoral behavior, Sexually related"], "all_classes": null, "length": 4566}
{"pred": "  The authors show that their learned policy generalizes better than existing solutions to unseen games by training a Seq2Seq model on the trajectories found by Go-Explore and then testing its performance on a set of 4,440 games that the model has not seen before. They find that the Go-Explore Seq2Seq model achieves a higher score than other methods, even though it does not use adm", "answers": ["promising results by solving almost half of the unseen games, most of the lost games are in the hardest set, where a very long sequence of actions is required for winning the game"], "all_classes": null, "length": 4964}
{"pred": "  Yes. The goal of investigating NLP gender bias specifically in the news broadcast domain and Anchor role is to understand the impact of gender representation in data on ASR performance and to question the impact of such a representation on a ASR system trained on these data.", "answers": ["create fair systems", " broadcast recordings are also a valuable source of data for the speech processing community, recent works uncovering gender bias in several natural language processing (NLP) tools"], "all_classes": null, "length": 4063}
{"pred": "  Yes, they build a predictive model to classify dogmatic posts from Reddit based on linguistic features.", "answers": ["logistic regression models", "logistic regression models based on unigram bag-of-words features (BOW), sentiment signals (SENT), the linguistic features from our earlier analyses (LING), and combinations of these features."], "all_classes": null, "length": 4208}
{"pred": "  Yes.", "answers": ["No", "Unanswerable"], "all_classes": null, "length": 4181}
{"pred": "  Yes. According to the article, the state-of-the-art system is MPAD.", "answers": ["doc2vec , CNN, DAN, Tree-LSTM, DRNN, LSTMN, C-LSTM, SPGK, WMD, S-WMD, Semantic-CNN, LSTM-GRNN, HN-ATT"], "all_classes": null, "length": 4169}
{"pred": "  They use a gradient-based method, integrated gradients (IG), to estimate the word importance in an input sentence precisely.", "answers": ["They measured the under-translated words with low word importance score as calculated by Attribution.\nmethod", "we ask ten human annotators to manually label the under-translated input words, and at least two annotators label each input-hypothesis pair"], "all_classes": null, "length": 4246}
{"pred": "  The model is applied to two datasets: (1) \"Conversations Gone Awry\" and (2) \"ChangeMyView\" from subreddit.", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4718}
{"pred": "  Yes.", "answers": ["No"], "all_classes": null, "length": 4117}
{"pred": "  The article uses the Europarl dataset for the in-domain training data and the WMT newstest 2014 dataset for the out-of-domain testing data. Therefore, the answer is: Europarl dataset.", "answers": ["Europarl corpus , WMT newstest 2014, News-Commentary-11, Wikipedia from WMT 2014, Multi-UN, EU-Bookshop, Rapid, Common-Crawl (WMT 2017)", "Europarl tests from 2006, 2007, 2008; WMT newstest 2014."], "all_classes": null, "length": 4184}
{"pred": "  Yes.", "answers": ["No", "No"], "all_classes": null, "length": 4484}
{"pred": "  A second-order co-occurrence matrix is a matrix that contains information about the similarity between pairs of words based on their co-occurrence patterns in a corpus of text.", "answers": ["frequencies of the other words which occur with both of them (i.e., second order co–occurrences)", "The matrix containing co-occurrences of the words which occur with the both words of every given pair of words."], "all_classes": null, "length": 4271}
{"pred": "  The article reports the following evaluation metrics:\n\n* Precision\n* Recall\n* F1 score\n* Accuracy\n\nTherefore, the answer is \"yes\".", "answers": ["precision, recall, F1 and accuracy", "precision, recall, F1 , accuracy "], "all_classes": null, "length": 4101}
{"pred": "  Unanswerable. The article does not provide information on the language of the data used for evaluation.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4770}
{"pred": "  Based on the article, the answer to the question \"How do they match annotators to instances?\" is:\n\nUnanswerable.\n\nThe article does not provide any information on how the authors match annotators to instances.", "answers": ["Annotations from experts are used if they have already been collected."], "all_classes": null, "length": 4377}
{"pred": "  The size of their dataset is approximately 2,100 texts and a total of 14,000 questions.", "answers": ["13,939"], "all_classes": null, "length": 4489}
{"pred": "  Yes", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4592}
{"pred": "  Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4457}
{"pred": "  According to the article, the following evaluation methods are used:\n\n* ROGUE\n* Human evaluation", "answers": ["Quantitative evaluation methods using ROUGE, Recall, Precision and F1.", "standard ROGUE metric, Recall, Precision and INLINEFORM0 scores for ROGUE-1,  INLINEFORM2 scores for ROGUE-2 and ROGUE-L"], "all_classes": null, "length": 4395}
{"pred": "  Based on the article, the baseline methods are:\n\n* NaiveNN\n* FANN\n* Blitzer et al. (BIBREF0)\n\nThese methods are compared to the proposed method, DAS, in the article.", "answers": ["(1) Naive, (2) mSDA BIBREF7, (3) NaiveNN, (4) AuxNN BIBREF4, (5) ADAN BIBREF16, (6) MMD", "non-domain-adaptive baseline with bag-of-words representations and SVM classifier, mSDA, non-domain-adaptive CNN trained on source domain, neural model that exploits auxiliary tasks, adversarial training to reduce representation difference between domains, variants of deep CNNs are used for encoding images and the MMDs of multiple layers are jointly minimized"], "all_classes": null, "length": 5063}
{"pred": "  Yes, their system's absolute accuracy is 59.4%.", "answers": ["59.4% on wikipedia dataset, 93.4% on peer-reviewed archive AI papers, 77.1%  on peer-reviewed archive Computation and Language papers, and 79.9% on peer-reviewed archive Machine Learning papers"], "all_classes": null, "length": 4203}
{"pred": "  The embedding techniques explored in the paper are:\n\n* Word embeddings (BIBREF9)\n* Second-order co-occurrence vectors (INLINEFORM0)\n* Integration of semantic similarity measures into second-order vectors (INLINEFORM1)\n* Thresholding experiments (INLINEFORM2-INLINEFORM3)\n* Using a threshold cutoff to include only those term pairs", "answers": ["Skip–gram, CBOW", "integrated vector-res, vector-faith, Skip–gram, CBOW"], "all_classes": null, "length": 4259}
{"pred": "  The two large-scale datasets used are:\n\n1. Disinformation articles published by websites known for producing low-credibility content, false and misleading news reports, and extreme propaganda and hoaxes, and flagged as such by reputable journalists and fact-checkers.\n2. Mainstream news articles from traditional news outlets that deliver factual and credible information.", "answers": ["US dataset, Italian dataset", "US dataset, Italian dataset"], "all_classes": null, "length": 4857}
{"pred": "  Unanswerable. The article does not provide information on how profile changes vary for influential leads and their followers over the social movement.", "answers": ["Influential leaders are more likely to change their profile attributes than their followers; the leaders do not change their usernames, while their followers change their usernames a lot; the leaders  tend to make new changes related to previous attribute values, while the followers make comparatively less related changes to previous attribute values."], "all_classes": null, "length": 5092}
{"pred": "  They show there is space for further improvement by conducting a human study that tested the model's performance on a random subset of named entity and common noun validation questions that the model could not answer correctly. The results of the human study suggest that there is still room for improvement beyond the results achieved by the model. Therefore, the answer is \"yes\".", "answers": [" by testing humans on a random subset of 50 named entity and 50 common noun validation questions that the psr ensemble could not answer correctly", "majority of questions that our system could not answer so far are in fact answerable"], "all_classes": null, "length": 4232}
{"pred": "  Yes, the article mentions that the evaluation metrics used in the task include the macro-averaged F-score.", "answers": ["precision, recall , F1 score"], "all_classes": null, "length": 4515}
{"pred": "  Unanswerable. The article does not provide information on the language or origin of the question-answer pairs used in the study.", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 7251}
{"pred": "  Unanswerable. The article does not provide information on the size of the data set used in the study.", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": "  Yes.", "answers": ["No", "No"], "all_classes": null, "length": 4071}
{"pred": "  The article does not provide information on the metrics used to establish that the proposed approach makes chatbots more knowledgeable and better at learning and conversation. Therefore, the question is unanswerable.", "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 5869}
{"pred": "  Yes. According to the article, the researchers use a combination of computational methods and human analysis to study the relationship between hate speech and online communities. They use machine learning algorithms to identify hate speech and analyze the distribution of function words to understand the cultural and social context of the language. Additionally, they use human coders to validate the machine-generated labels and assess the reliability and validity of the measurement tools.", "answers": ["Domain experts and fellow researchers can provide feedback on questions and help with dynamically revising them., connect to multiple disciplines, dual use", "Modeling considerations:  the variables (both predictors and outcomes)  are rarely simply binary or categorical;  using a particular classification scheme means deciding which variations are visible,; Supervised and unsupervised learning are the most common approaches to learning from data;  the unit of text that we are labeling (or annotating, or coding), either automatic or manual, can sometimes be different than one's final unit of analysis."], "all_classes": null, "length": 8530}
{"pred": "  Yes. According to the article, the researchers use \"rich online data\" for their analysis.", "answers": ["Unanswerable"], "all_classes": null, "length": 8508}
{"pred": "  Yes. According to the article, the best performance was achieved by the HealthCare sector.", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 10349}
{"pred": "  Yes. The article provides information about the background of the researchers involved in the study of text analysis, including their disciplinary backgrounds and research practices.", "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": "  Yes. According to the article, the following machine learning methods are used in experiments:\n\n* \"experiment on the annotated data using various machine learning methods\"\n* \"propose several novel feature sets and identify configurations that run best in in-domain and cross-domain scenarios\"", "answers": ["Structural Support Vector Machine", "SVMhmm "], "all_classes": null, "length": 14468}
{"pred": "  Yes", "answers": ["Yes", "Yes"], "all_classes": null, "length": 12950}
{"pred": "  The state of the art described in the paper is the ability to interact with one or more people or chatbots in a multi-party chat, referred to as MPCS.", "answers": ["ELIZA,  PARRY, A.L.I.C.E., Cleverbot"], "all_classes": null, "length": 13395}
{"pred": "  Yes, they use a corpus of state speeches delivered during the annual UN General Debate.", "answers": ["corpus of state speeches delivered during the annual UN General Debate", "corpus of state speeches delivered during the annual UN General Debate"], "all_classes": null, "length": 8647}
{"pred": "  The article mentions the use of \"multiscale graph partitioning for text analysis: description of the framework\" which combines \"text-embedding, geometric graph construction and multi-resolution community detection to identify, rather than impose, content-based clusters from free, unstructured text in an unsupervised manner.\" Therefore, the answer is \"text embedding methods\" or \"text-embedding, geometric graph construction,", "answers": ["Document to Vector (Doc2Vec)", "Doc2Vec, PV-DBOW model"], "all_classes": null, "length": 8196}
{"pred": "  Yes. According to the article, the ML methods aim to identify argument components such as premises, backing, rebuttals, and refutations.", "answers": ["claim, premise, backing, rebuttal, and refutation", "claim, premise, backing, rebuttal, refutation"], "all_classes": null, "length": 14472}
{"pred": "  Unanswerable. The article does not provide information on the language of the data or the results of the analysis, so it is unable to determine whether the results are reported only on English data.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 14465}
{"pred": "  The article does not provide information on the datasets used. Therefore, the answer is \"unanswerable\".", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 13401}
{"pred": "  Yes.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 12952}
{"pred": "  The node in the network approach represents a state's positions or preferences in international politics.", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 8641}
{"pred": "  The answer is: The article does not provide information on how the data in the new corpus is sourced. Therefore, the question is unanswerable.", "answers": ["user comments to newswire articles or to blog posts, forum posts, blog posts, newswire articles", "refer to each article, blog post, comment, or forum posts as a document"], "all_classes": null, "length": 14481}
{"pred": "  They use a computational linguistic framework to identify discussions of LGBTQ people in the New York Times.", "answers": ["act paragraphs containing any word from a predetermined list of LGTBQ terms "], "all_classes": null, "length": 12970}
{"pred": "  Yes", "answers": ["No", "No"], "all_classes": null, "length": 8509}
{"pred": "  Unanswerable. The article does not provide information on the specific issues tackled by the researchers beyond the examples provided in the passage.", "answers": ["identifying the questions we wish to explore, Can text analysis provide a new perspective on a “big question” that has been attracting interest for years?, How can we explain what we observe?, hope to connect to multiple disciplines"], "all_classes": null, "length": 8555}
{"pred": "  Based on the article provided, the evaluation metrics looked at are:\n\n* Time taken to respond to a specific utterance by the chatbot\n* Whether the chatbot answered correctly given a pre-defined set of known dialogues\n* Whether the chatbot answered in a reasonable time\n* Amount of computing resources consumed to answer a specific utterance.\n\nUnanswerable: What is the median time for the eight", "answers": ["precision, recall, F1 and accuracy", "Response time, resource consumption (memory, CPU, network bandwidth), precision, recall, F1, accuracy."], "all_classes": null, "length": 13391}
{"pred": "  Yes.", "answers": ["No", "Yes"], "all_classes": null, "length": 8643}
{"pred": "  Unanswerable. The article does not provide information on the challenges posed by different registers and domains to the task of argumentation mining.", "answers": ["linguistic variability"], "all_classes": null, "length": 14469}
{"pred": "  The datasets were annotated using a carefully designed and rigorous language-agnostic translation and annotation protocol.", "answers": ["1. Each annotator must assign an integer score between 0 and 6 (inclusive) indicating how semantically similar the two words in a given pair are. A score of 6 indicates very high similarity (i.e., perfect synonymy), while zero indicates no similarity., 2. Each annotator must score the entire set of 1,888 pairs in the dataset.,  able to use external sources (e.g. dictionaries, thesauri, WordNet) if required, not able to communicate with each other during the annotation process"], "all_classes": null, "length": 14722}
{"pred": "  Unanswerable. The article does not provide information on what argumentation phenomena encounter in actual data are accounted for by this work.", "answers": ["Unanswerable"], "all_classes": null, "length": 14471}
{"pred": "  Yes, the 12 languages covered are:\n\n1. English\n2. German\n3. Italian\n4. Russian\n5. Mandarin\n6. French\n7. Spanish\n8. Portuguese\n9. Dutch\n10. Swedish\n11. Danish\n12. Finnish", "answers": ["Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese", "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese"], "all_classes": null, "length": 14660}
