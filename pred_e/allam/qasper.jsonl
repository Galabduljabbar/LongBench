{"pred": " The answer is \"Facebook pages\" because the question asks which Facebook pages were looked at. ", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": " The answer is \"unanswerable\" because the article does not provide information about the type of latent context used to predict instructor intervention. ", "answers": ["the series of posts that trigger an intervention"], "all_classes": null, "length": 3732}
{"pred": " Kappa statistics ", "answers": ["F-score, Kappa", "Unanswerable"], "all_classes": null, "length": 3544}
{"pred": " The baselines were models that use plain stacked LSTMs, models with different INLINEFORM0, models without INLINEFORM1, and models that integrate lower contexts via peephole connections. ", "answers": ["(i) models that use plain stacked LSTMs, (ii) models with different INLINEFORM0, (iii) models without INLINEFORM1, (iv) models that integrate lower contexts via peephole connections"], "all_classes": null, "length": 3224}
{"pred": " unanswerable ", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2284}
{"pred": " unanswerable ", "answers": ["No"], "all_classes": null, "length": 3843}
{"pred": " unanswerable ", "answers": ["Yes", "No"], "all_classes": null, "length": 2317}
{"pred": " The article does not provide a list of the datasets used in evaluation. It only mentions that there are four languages and that the data was part of SemEval-2016 Challenge Task 5. ", "answers": ["SemEval-2016 Challenge Task 5 BIBREF27 , BIBREF28", " English reviews ,  restaurant reviews from four different languages (Spanish, Turkish, Dutch, Russian)"], "all_classes": null, "length": 2720}
{"pred": " The answer is \"unanswerable\" because the article does not provide specific information about the improvements of small-scale unbalanced datasets when sentence representation is enhanced with topic information. ", "answers": ["7.36% on accuracy and 9.69% on F1 score", "it has 0.024 improvement in accuracy comparing to ELMO Only and  0.006 improvement in F1 score  comparing to ELMO Only too"], "all_classes": null, "length": 2890}
{"pred": " They use datasets with transcribed text. ", "answers": ["They use text transcription.", "both"], "all_classes": null, "length": 3198}
{"pred": " CLUTO and Carrot2 Lingo. ", "answers": ["CLUTO, Carrot2 Lingo", "simple clustering algorithm which uses the cosine similarity between word embeddings"], "all_classes": null, "length": 3039}
{"pred": " BERT ", "answers": ["Bert + Unanswerable", "CNN, BERT"], "all_classes": null, "length": 3845}
{"pred": " The pivot-based baselines are pivoting, which translates source to pivot language and then to target in two steps. ", "answers": ["pivoting, pivoting$_{\\rm m}$", "firstly translates a source language into the pivot language which is later translated to the target language"], "all_classes": null, "length": 3815}
{"pred": " The answer is \"Friends and EmotionPush\". ", "answers": ["Friends, EmotionPush", "EmotionLines BIBREF6"], "all_classes": null, "length": 3178}
{"pred": " The evaluation protocols allow researchers to investigate the performance of different methods in various scenarios and study the effects of channels, duration and phrase text on the performance. ", "answers": ["three experimental setups with different numbers of speakers in the evaluation set, three experimental setups with different number of speaker in the evaluation set are defined,  first one, respondents with at least 17 recording sessions are included to the evaluation set, respondents with 16 sessions to the development and the rest of respondents to the background set, second setup, respondents with at least 8 sessions are included to the evaluation set, respondents with 6 or 7 sessions to the development and the rest of respondents to the background set"], "all_classes": null, "length": 3880}
{"pred": " unanswerable ", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": " The article does not provide specific information about the amount of training data from the non-English language used by the system. ", "answers": ["No data. Pretrained model is used."], "all_classes": null, "length": 3409}
{"pred": " Attention is different from alignment in cases where it captures information beyond alignment, such as in the translation of verbs. ", "answers": ["For certain POS tags, e.g. VERB, PRON.", "most word alignments only involve one or a few words, attention can be distributed more freely"], "all_classes": null, "length": 3366}
{"pred": " The article does not provide information about the specific model used for end-to-end speech recognition. Therefore, the answer is \"unanswerable\". ", "answers": ["RNN", " Recurrent Neural Network (RNN)"], "all_classes": null, "length": 1644}
{"pred": " unanswerable ", "answers": ["CNN modelBIBREF0, Stanford CRF modelBIBREF21", "Bam et al. SVM, Ma and Hovy w/glove, Lample et al. w/fastText, Lample et al. w/word2vec"], "all_classes": null, "length": 2836}
{"pred": " unanswerable ", "answers": ["Unanswerable"], "all_classes": null, "length": 3885}
{"pred": " Around 500 different workers were involved in the annotation. ", "answers": ["about 500", "Unanswerable"], "all_classes": null, "length": 3831}
{"pred": " The models used in the experiment are SVM, BiLSTM, and CNN. ", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": " The answer is \"yes\". ", "answers": ["BiLSTM, BiLSTM-CNN, BiLSTM-CRF, BiLSTM-CNN-CRF", "BiLSTMBIBREF14, BiLSTM+CNNBIBREF20, BiLSTM+CRFBIBREF1, BiLSTM+CNN+CRFBIBREF2, CNN modelBIBREF0 and Stanford CRF modelBIBREF21"], "all_classes": null, "length": 2838}
{"pred": " The vocabulary of word-like or phoneme-like units is automatically discovered through unsupervised approaches such as unsupervised term discovery (UTD) and acoustic unit discovery (AUD). ", "answers": ["Zero Resource Toolkit (ZRTools) BIBREF7"], "all_classes": null, "length": 3381}
{"pred": " BERTBase ", "answers": ["BERTbase", "BERTbase"], "all_classes": null, "length": 3852}
{"pred": " The question asks which keyphrase extraction models were reassessed. The answer to this is \"five keyphrase extraction models\". ", "answers": ["Answer with content missing: (LVL1, LVL2, LVL3) \n- Stanford CoreNLP\n- Optical Character Recognition (OCR) system, ParsCIT \n- further abridge the input text from level 2 preprocessed documents to the following: title, headers, abstract, introduction, related work, background and conclusion."], "all_classes": null, "length": 1822}
{"pred": " Yes ", "answers": ["Yes", "Yes"], "all_classes": null, "length": 2077}
{"pred": " The article does not provide information about the specific datasets used. Therefore, the answer is \"unanswerable\". ", "answers": ["MIMIC-III", "MIMIC-III"], "all_classes": null, "length": 2992}
{"pred": " unanswerable ", "answers": ["Level A: 14100 Tweets\nLevel B: 4640 Tweets\nLevel C: 4089 Tweets"], "all_classes": null, "length": 2255}
{"pred": " The answer is \"unanswerable\" because the article does not provide specific information about what useful information attention captures. ", "answers": ["it captures other information rather than only the translational equivalent in the case of verbs", "Alignment points of the POS tags."], "all_classes": null, "length": 3372}
{"pred": " The baselines are bag-of-words (BOW), term frequency–inverse document frequency (TFIDF), and neural-based word embedding. The classification models are including Logistic Regression (LR), Random Forest (RF), TextCNN with initial word embedding as GloVe, and the proposed model. ", "answers": ["BOW-LR, BOW-RF. TFIDF-RF, TextCNN, C-TextCNN", "bag-of-words (BOW), term frequency–inverse document frequency (TFIDF), neural-based word embedding, Logistic Regression (LR), Random Forest (RF), TextCNN BIBREF10 with initial word embedding as GloVe"], "all_classes": null, "length": 3181}
{"pred": " unanswerable ", "answers": ["Unanswerable", "48,705"], "all_classes": null, "length": 3307}
{"pred": " The question asks for the architecture of their model, but the article does not provide information about the architecture. Therefore, the answer is \"unanswerable\". ", "answers": ["follow the architecture in berard2018end, but have 3 decoder layers like that in pino2019harnessing"], "all_classes": null, "length": 2424}
{"pred": " The data used in this study are the Penn Treebank (PTB) and WikiText2 (WT-2) datasets. ", "answers": [" Penn Treebank, WikiText2", "Penn Treebank (PTB) , WikiText2 (WT-2)"], "all_classes": null, "length": 3302}
{"pred": " unanswerable ", "answers": ["No", "No"], "all_classes": null, "length": 3158}
{"pred": " Prior knowledge distillation techniques require the student and teacher models to share the same vocabulary and output space, which is not possible when the student model has a different vocabulary. ", "answers": ["While there has been existing work on reducing NLP model vocabulary sizes BIBREF15, distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes.", "distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes."], "all_classes": null, "length": 3570}
{"pred": " The baseline method is word2vec. ", "answers": ["using word2vec to create features that are used as input to the SVM", "use the word2vec algorithm, create several unsupervised hand-crafted features, generate document vectors and feed them as input into the support vector machines (SVM) approach"], "all_classes": null, "length": 3820}
{"pred": " The ancient Chinese dataset comes from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era. ", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": " unanswerable ", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": " The answer is \"words embeddings, style, and morality features\". ", "answers": ["words embeddings, style, and morality features", "words embeddings, style, and morality features"], "all_classes": null, "length": 2091}
{"pred": " XNLI dataset and Universal Dependencies v2.4 ", "answers": ["United Nations Parallel Corpus, IIT Bombay corpus, OpenSubtitles 2018"], "all_classes": null, "length": 3405}
{"pred": " The article does not provide information about the languages for which word embeddings are built. Therefore, the answer is \"unanswerable\". ", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 2009}
{"pred": " unanswerable ", "answers": ["No", "No"], "all_classes": null, "length": 1899}
{"pred": " The collection of COVID-19 literature is very large, containing over 45,000 scholarly articles. ", "answers": ["45,000 scholarly articles, including over 33,000 with full text"], "all_classes": null, "length": 2150}
{"pred": " The proposed model is compared to a comprehensive set of baseline models, including some that had never been applied to the medical term abbreviation disambiguation task. ", "answers": ["support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self", "support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self"], "all_classes": null, "length": 2895}
{"pred": " The number of electrodes used on the subject in EEG sessions is not provided in the article. ", "answers": ["1913 signals", "Unanswerable"], "all_classes": null, "length": 2361}
{"pred": " The article does not provide a list of the different modules in Macaw, so the answer is \"unanswerable\". ", "answers": ["Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation", "Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation"], "all_classes": null, "length": 1701}
{"pred": " unanswerable ", "answers": ["Unanswerable"], "all_classes": null, "length": 1913}
{"pred": " The article does not provide information about the accents present in the corpus. Therefore, the answer is \"unanswerable\". ", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": " SQuAD dataset ", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": " The dataset they train their models on is the SemEval 2010 task 8 benchmark data. ", "answers": ["relation classification dataset of the SemEval 2010 task 8", "SemEval 2010 task 8 BIBREF8"], "all_classes": null, "length": 2393}
{"pred": " The size of the corpora is not specified in the article. ", "answers": ["2174000000, 989000000", "2174 million tokens for English and 989 million tokens for Russian"], "all_classes": null, "length": 2958}
{"pred": " The qualitative experiments performed on benchmark datasets involve comparing the performance of GM$\\_$KL, w2g, and w2gm approaches. The metrics used for comparison include MaxCos, AvgCos, KL$\\_$approx, and KL$\\_$comp. The results show that GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset. For most of the datasets, GM$\\_$KL achieves significantly better correlation score than w2g and w2gm approaches. ", "answers": ["Spearman correlation values of GM_KL model evaluated on the benchmark word similarity datasets.\nEvaluation results of GM_KL model on the entailment datasets such as entailment pairs dataset created from WordNet, crowdsourced dataset of 79 semantic relations labelled as entailed or not and annotated distributionally similar nouns dataset.", "Given a query word and component id, the set of nearest neighbours along with their respective component ids are listed"], "all_classes": null, "length": 2220}
{"pred": " The method improvements of F1 for paraphrase identification are +0.58 for MRPC and +0.73 for QQP. ", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": " unanswerable ", "answers": ["reducing the variance of an estimator,  EGL method in BIBREF3 is almost the same as Eq. ( EQREF8 ), except the gradient's norm is not squared in BIBREF3", " Fisher Information Ratio"], "all_classes": null, "length": 1671}
{"pred": " The architecture of the encoder is not described in the article. ", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": " The question is asking what kind of questions are present in the dataset. The answer to this is \"yes\". ", "answers": ["These 8 tasks require different competencies and a different level of understanding of the document to be well answered"], "all_classes": null, "length": 3817}
{"pred": " Yes ", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": " 53 ", "answers": ["53 documents", "53 documents"], "all_classes": null, "length": 2667}
{"pred": " The article does not provide information about where the recipes are from, so the answer is \"unanswerable\". ", "answers": ["from Food.com"], "all_classes": null, "length": 2649}
{"pred": " The question asks how the authors evaluate their resulting word embeddings. The answer is \"intrinsic evaluations consisting of word similarity and word analogy tasks\" because these are the methods used to evaluate the quality of the embeddings. ", "answers": ["We also evaluate all five models on downstream tasks from the VecEval suite BIBREF13 , using only the tasks for which training and evaluation data is freely available: chunking, sentiment and question classification, and natural language identification (NLI). The default settings from the suite are used, but we run only the fixed settings, where the embeddings themselves are not tunable parameters of the models, forcing the system to use only the information already in the embeddings."], "all_classes": null, "length": 2083}
{"pred": " unanswerable ", "answers": ["Adv. Heart Disease, Adv. Lung Disease, Alcohol Abuse, Chronic Neurologic Dystrophies, Dementia, Depression, Developmental Delay, Obesity, Psychiatric disorders and Substance Abuse"], "all_classes": null, "length": 1651}
{"pred": " The article does not provide information about the average length of essays in the corpus. So, it is unanswerable. ", "answers": ["204 tokens", "Most texts, however, range roughly from 150 to 250 tokens."], "all_classes": null, "length": 1898}
{"pred": " The textual patterns are extracted from a corpus of annotated corrections, using error type distributions. ", "answers": ["(VVD shop_VV0 II, VVD shopping_VVG II)", "patterns for generating all types of errors"], "all_classes": null, "length": 2133}
{"pred": " unanswerable ", "answers": ["No", "No"], "all_classes": null, "length": 3974}
{"pred": " unanswerable ", "answers": ["Random Kitchen Sink method uses a kernel function to map data vectors to a space where linear separation is possible.", "explicitly maps data vectors to a space where linear separation is possible, RKS method provides an approximate kernel function via explicit mapping"], "all_classes": null, "length": 2361}
{"pred": " The question asks what other models are compared to in the article. The answer is \"Big Transformer model\" and a variant where token embeddings are shared between encoder and decoder. ", "answers": ["BIBREF11 , BIBREF26 "], "all_classes": null, "length": 1912}
{"pred": " The results show that error detection performance is substantially improved by making use of artificially generated data, created by any of the described methods. ", "answers": ["Combining  pattern based and Machine translation approaches gave the best overall F0.5 scores. It was 49.11 for FCE dataset  , 21.87 for the first annotation of  CoNLL-14, and 30.13 for the second annotation of CoNLL-14. "], "all_classes": null, "length": 2164}
{"pred": " The question asks what other tasks the method was tested on. The answer is \"unanswerable\" because the article does not provide information about other tasks that the method was tested on. ", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": " The experts used for annotation were recruited from crowdworkers who have been conferred \"master\" status and are located within the United States of America. ", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": " The answer is \"yes\". ", "answers": ["Maximum Entropy, Weighted Finite State Transducer (WFST),  support vector machines (SVM), conditional random fields (CRF)", "Maximum matching, Hidden Markov model , Maximum Entropy, Conditional Random Fields , Support Vector Machines"], "all_classes": null, "length": 3471}
{"pred": " The question asks what NER models were evaluated. The answer is \"Stanford NER, spaCy 2.0, and a recurrent model similar to BIBREF13 , BIBREF14 that uses bidirectional LSTM cells for character-based feature extraction and CRF\". ", "answers": ["Stanford NER, spaCy 2.0 , recurrent model with a CRF top layer", "Stanford NER, spaCy 2.0, recurrent model with a CRF top layer"], "all_classes": null, "length": 2759}
{"pred": " WN18 and FB15K. ", "answers": ["WordNet BIBREF0, Freebase BIBREF1, WN18 (a subset of WordNet) BIBREF24 , FB15K (a subset of Freebase) BIBREF2"], "all_classes": null, "length": 3367}
{"pred": " LastStateRNN ", "answers": [" LastStateRNN, AvgRNN, AttentionRNN", "LastStateRNN, AvgRNN, AttentionRNN "], "all_classes": null, "length": 2823}
{"pred": " The question asks what cyberbullying topics were addressed. The answer is \"personal attack, racism, and sexism\". ", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": " unanswerable ", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3634}
{"pred": " The article does not provide the name of the dataset used for this study. ", "answers": ["BIBREF12 , BIBREF13", "an existing, annotated Twitter dataset that was constructed based on a hierarchical model of depression-related symptoms BIBREF12 , BIBREF13"], "all_classes": null, "length": 1939}
{"pred": " The Nguni languages are similar to each other and harder to distinguish. The same is true of the Sotho languages. ", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": " The sentiment analysis dataset used is the IMDb dataset of movie reviews. ", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": " The system had an accuracy and F1-score of 89.6% and 89.2%, respectively. ", "answers": ["accuracy and F1-score of 89.6% and 89.2%, respectively", "accuracy and F1-score of 89.6% and 89.2%, respectively"], "all_classes": null, "length": 3313}
{"pred": " K-means, LEM, and DPEMM. ", "answers": ["K-means, LEM BIBREF13, DPEMM BIBREF14", "K-means, LEM, DPEMM"], "all_classes": null, "length": 3841}
{"pred": " The answer is based on the fact that the PolyResponse restaurant search engine is currently deployed in 8 languages. ", "answers": ["English, German, Spanish, Mandarin, Polish, Russian, Korean and Serbian", "English (Edinburgh), German (Berlin), Spanish (Madrid), Mandarin (Taipei), Polish (Warsaw), Russian (Moscow), Korean (Seoul), and Serbian (Belgrade)"], "all_classes": null, "length": 2754}
{"pred": " The sources of the datasets are not provided in the article. ", "answers": ["rupnik2016news", "rupnik2016news, Deutsche Welle's news website"], "all_classes": null, "length": 3160}
{"pred": " The question asks whether the lexicon is the same for all languages. The article does not provide information about whether or not the lexicon is the same for all languages, so it's unanswerable. ", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1868}
{"pred": " InferSent and Universal Sentence Encoder ", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": " The answer is \"SentEval\". ", "answers": ["MR, CR, SUBJ, MPQA, SST, TREC, MRPC", "MR: Sentiment prediction for movie reviews snippets on a five start scale BIBREF25.\n\nCR: Sentiment prediction of customer product reviews BIBREF26.\n\nSUBJ: Subjectivity prediction of sentences from movie reviews and plot summaries BIBREF27.\n\nMPQA: Phrase level opinion polarity classification from newswire BIBREF28.\n\nSST: Stanford Sentiment Treebank with binary labels BIBREF29.\n\nTREC: Fine grained question-type classification from TREC BIBREF30.\n\nMRPC: Microsoft Research Paraphrase Corpus from parallel news sources BIBREF31.", "Semantic Textual Similarity, sentiment prediction, subjectivity prediction, phrase level opinion polarity classification, Stanford Sentiment Treebank, fine grained question-type classification."], "all_classes": null, "length": 3861}
{"pred": " unanswerable ", "answers": ["33,663", "33,663 distinct review keywords "], "all_classes": null, "length": 3305}
{"pred": " The additional features proposed are the context tweets, which provide significant contextual information for understanding a tweet. ", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": " Yes ", "answers": ["No", "No"], "all_classes": null, "length": 1906}
{"pred": " The article describes how the authors generated maps that reflect demographic, linguistic, and psycholinguistic properties of the population represented in their dataset. However, it does not mention building a model to automatically detect these dimensions. Therefore, the answer is \"unanswerable\". ", "answers": ["No", "No"], "all_classes": null, "length": 1443}
{"pred": " The best performing model among the author's submissions is the ensemble of Logistic Regression, CNN, and BERT, with an F1 score of 0.673 on the test set. ", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": " The corpus used for the task is DTA18 and DTA19, which are subparts of the DTA corpus. ", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": " The size of the dataset is not provided in the article. ", "answers": ["13,757", "10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs"], "all_classes": null, "length": 3704}
{"pred": " The classifiers trained are K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron (MLP). ", "answers": ["KNN\nRF\nSVM\nMLP", " K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), Multi-layer Perceptron (MLP)"], "all_classes": null, "length": 1639}
{"pred": " unanswerable ", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": " The article does not provide a specific answer to this question. ", "answers": ["An output layer for each task", "Multi-tasking is addressed by neural sequence tagger based on LSTM-CRF and linguistic features, while multi-granularity is addressed by ensemble of LSTM-CRF and BERT."], "all_classes": null, "length": 1514}
{"pred": " The CORD-19 dataset is a collection of over 45,000 scholarly articles about COVID-19, SARS-CoV-2, and related coronaviruses. ", "answers": ["which contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses", "contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses"], "all_classes": null, "length": 2156}
{"pred": " The real-life dataset contains 26972 sentences. ", "answers": ["26972", "26972 sentences"], "all_classes": null, "length": 3040}
{"pred": " unanswerable ", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2533}
{"pred": " unanswerable ", "answers": ["In pass-through, the recognizer passes on the possibly misspelled word, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.", "Pass-through passes the possibly misspelled word as is, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.", "Backoff to \"a\" when an UNK-predicted word is encountered, backoff to a more generic word recognition model when the model predicts UNK"], "all_classes": null, "length": 4210}
{"pred": " The corpus used to learn behavior is a dataset of recorded dialogues. ", "answers": ["DSTC2", "The manual transcriptions of the DSTC2 training set "], "all_classes": null, "length": 4820}
{"pred": " BLEU-4, NIST-4, and ROUGE-4. ", "answers": ["BLEU-4, NIST-4, ROUGE-4", "BLEU-4, NIST-4, ROUGE-4"], "all_classes": null, "length": 4460}
{"pred": " unanswerable ", "answers": ["SVM , CNN , GRU , CNN/GRU+law, r-net , AoA ", "SVM with lexical features in accordance with previous works BIBREF16 , BIBREF17 , BIBREF1 , BIBREF15 , BIBREF4, attention-based method BIBREF3 and other methods we deem important, some off-the-shelf RC models, including r-net BIBREF5 and AoA BIBREF6 , which are the leading models on SQuAD leaderboard"], "all_classes": null, "length": 4106}
{"pred": " The answer is \"yes\". ", "answers": ["POS, gender/number and stem POS"], "all_classes": null, "length": 4719}
{"pred": " The semantically related words take larger values along the dimension that is aligned with the predefined concepts from the lexicon. ", "answers": ["dimension corresponding to the concept that the particular word belongs to"], "all_classes": null, "length": 6182}
{"pred": " The question is not answered in the article. The answer is \"unanswerable\". ", "answers": ["English , German, French"], "all_classes": null, "length": 4173}
{"pred": " unanswerable ", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 4465}
{"pred": " The two news domains that are country-independent are mainstream and disinformation. ", "answers": ["mainstream news and disinformation", "mainstream and disinformation news"], "all_classes": null, "length": 4856}
{"pred": " The experiments are performed on CoinCollector and CookingWorld games. ", "answers": ["CoinCollector , CookingWorld ", "CoinCollector, CookingWorld"], "all_classes": null, "length": 4924}
{"pred": " F INLINEFORM1 @ INLINEFORM2 and F INLINEFORM3 @ INLINEFORM4 ", "answers": ["average unique predictions, randomly sample 2000 decoder hidden states at INLINEFORM4 steps following a delimiter ( INLINEFORM5 ) and apply an unsupervised clustering method (t-SNE BIBREF35 )"], "all_classes": null, "length": 4581}
{"pred": " The model captures biases from data collection and annotation rules. ", "answers": ["Data annotation biases where tweet containing disrespectful words are annotated as hate or offensive without any presumption about the social context of tweeters"], "all_classes": null, "length": 4111}
{"pred": " The article does not provide specific future possible improvements. ", "answers": ["rther constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions", "to further constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions"], "all_classes": null, "length": 4704}
{"pred": " The additive modification to the objective function is introduced by adding an additive term to the cost function for any one of the words of concept word-groups. This term favors an increase for the value of this word's embedding vector dimension corresponding to the concept that the particular word belongs to. For words that do not belong to any one of the word-groups, the cost term is left untouched. ", "answers": ["The cost function for any one of the words of concept word-groups is modified by the introduction of an additive term to the cost function. . Each embedding vector dimension is first associated with a concept. For a word belonging to any one of the word-groups representing these concepts, the modified cost term favors an increase for the value of this word's embedding vector dimension corresponding to the concept that the particular word belongs to,", "An additive term added to the cost function for any one of the words of concept word-groups"], "all_classes": null, "length": 6244}
{"pred": " unanswerable ", "answers": ["Yes", "Yes"], "all_classes": null, "length": 7837}
{"pred": " The document-level encoder is novel because it is able to encode a document and obtain representations for its sentences, which is important for the summarization task. ", "answers": ["Bert model have a maximum length of 512; we overcome this limitation by adding more position embeddings, we insert external [cls] tokens at the start of each sentence, and each [cls] symbol collects features for the sentence preceding it, document representations are learned hierarchically"], "all_classes": null, "length": 4404}
{"pred": " Their model improves interpretability by using sparse attention, which allows for more confident and specialized attention heads, and by automatically learning different sparsity patterns for each head. ", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": " unanswerable ", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 5331}
{"pred": " The question asks how many users the study looks at. The article states that the study looks at over 20,000 blog users, so the answer is \"over 20,000\". ", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": " unanswerable ", "answers": ["sentence"], "all_classes": null, "length": 4369}
{"pred": " The labels for antisocial events in the datasets are \"personal attacks\" and \"rude or hostile behavior\". ", "answers": ["The Conversations Gone Awry dataset is labelled as either containing a personal attack from withint (i.e. hostile behavior by one user in the conversation directed towards another) or remaining civil throughout. The Reddit Change My View dataset is labelled with whether or not a coversation eventually had a comment removed by a moderator for violation of Rule 2: \"Don't be rude or hostile to others users.\""], "all_classes": null, "length": 4779}
{"pred": " The article does not provide information about the specific datasets used in the experiment. ", "answers": ["build a new one, collect INLINEFORM0 cases from China Judgments Online"], "all_classes": null, "length": 4109}
{"pred": " They measure style transfer success by calculating the root mean square error between the average human score and the desired values (+1 if more formal, -1 if more informal, 0 if neither). ", "answers": ["Unanswerable"], "all_classes": null, "length": 4572}
{"pred": " unanswerable ", "answers": ["absolute improvement of 18.2% over the Pointer-Gen baseline"], "all_classes": null, "length": 4091}
{"pred": " unanswerable ", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": " The model used in this paper is a joint model that integrates visual features learned through Inception V3 with textual features learned through a biLSTM. ", "answers": ["visual model is based on fine-tuning an Inception V3 model BIBREF1 over visual renderings of documents, while our textual model is based on a hierarchical biLSTM. We further combine the two into a joint model. , neural network models", "Inception V3, biLSTM"], "all_classes": null, "length": 4217}
{"pred": " The baseline is the M2M Transformer model (b3). ", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": " The global network features include Number of Strongly Connected Components (SCC), Size of the Largest Strongly Connected Component (LSCC), Number of Weakly Connected Components (WCC), Size of the Largest Weakly Connected Component (LWCC), Diameter of the Largest Weakly Connected Component (DWCC), Average Clustering Coefficient (CC), Main K-core Number (KC), Density (d), and Structural virality of the largest weakly connected component (SV). ", "answers": ["Number of Strongly Connected Components (SCC), Size of the Largest Strongly Connected Component (LSCC), Number of Weakly Connected Components (WCC), Size of the Largest Weakly Connected Component (LWCC), Diameter of the Largest Weakly Connected Component (DWCC), Average Clustering Coefficient (CC), Main K-core Number (KC), Density (d)"], "all_classes": null, "length": 4906}
{"pred": " unanswerable ", "answers": ["QA PGNet, Multi-decoder QA PGNet with lookup table embedding", "QA PGNet and Multi-decoder QA PGNet"], "all_classes": null, "length": 4600}
{"pred": " unanswerable ", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": " The answer is based on the fact that the proposed approach adapts an off-the-shelf reward learning algorithm from RL for joint data manipulation learning and model training. ", "answers": ["BIBREF7", " reward learning algorithm BIBREF7"], "all_classes": null, "length": 4841}
{"pred": " BioASQ ", "answers": ["BioASQ  dataset", "A dataset provided by BioASQ consisting of questions, gold standard documents, snippets, concepts  and ideal and ideal answers."], "all_classes": null, "length": 6810}
{"pred": " The manual Pyramid scores are used to evaluate the effectiveness of Rouge and the proposed method (Sera) by analyzing the correlations with semi-manual human judgments. ", "answers": [" higher tiers of the pyramid", "following the pyramid framework, we design an annotation scheme"], "all_classes": null, "length": 4771}
{"pred": " unanswerable ", "answers": ["Given the contribution matrix, we can obtain the word importance of each input word to the entire output sentence. ", "They compute the gradient of the output at each time step with respect to the input words to decide the importance."], "all_classes": null, "length": 4253}
{"pred": " unanswerable ", "answers": ["BIBREF9 , BIBREF8 ", "BIBREF9 , BIBREF8"], "all_classes": null, "length": 4855}
{"pred": " The article does not provide specific values for the improvement in classification performance. ", "answers": ["Low data: SST-5, TREC, IMDB around 1-2 accuracy points better than baseline\nImbalanced labels: the improvement over the base model increases as the data gets more imbalanced, ranging from around 6 accuracy points on 100:1000 to over 20 accuracy points on 20:1000"], "all_classes": null, "length": 4880}
{"pred": " multiple choice question answering ", "answers": ["MULTIPLE CHOICE QUESTION ANSWERING", "multiple-choice"], "all_classes": null, "length": 6396}
{"pred": " The answer is based on the fact that the article mentions two widely-studied datasets provided by Waseem and Hovey, as well as Davidson et al. ", "answers": ["Waseem-dataset, Davidson-dataset,", "Waseem and Hovey BIBREF5, Davidson et al. BIBREF9"], "all_classes": null, "length": 4090}
{"pred": " Transformer and RNN-Search. ", "answers": [" Transformer BIBREF1 model and the conventional RNN-Search model BIBREF0", "Transformer, RNN-Search model"], "all_classes": null, "length": 4240}
{"pred": " unanswerable ", "answers": ["average unique predictions, illustrate the difference of predictions between our proposed models, we show an example chosen from the KP20k validation set"], "all_classes": null, "length": 4576}
{"pred": " unanswerable ", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": " The article compares 6-layers and 9-layers sMBR models, as well as CE models. ", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": " unanswerable ", "answers": ["Yes"], "all_classes": null, "length": 4073}
{"pred": " KAR is an end-to-end MRC model. ", "answers": ["Lexicon Embedding Layer, Context Embedding Layer, Coarse Memory Layer, Refined Memory Layer, Answer Span Prediction Layer"], "all_classes": null, "length": 4133}
{"pred": " Improving the sensationalism scorer and investigating the applications of dynamic balancing methods between RL and MLE in textGAN. ", "answers": ["ethical questions about generating sensational headlines, which can be further explored,  improving the sensationalism scorer, investigating the applications of dynamic balancing methods between RL and MLE"], "all_classes": null, "length": 4104}
{"pred": " The dataset models character's profiles by collecting Human Level Attributes (HLAs) from TV Tropes, which are determined by human viewers and their impressions of the characters. The attributes are correlated with human-like characteristics, and the dataset is filtered to keep only characters with at least five HLAs. ", "answers": ["attributes are determined by human viewers and their impressions of the characters, and are correlated with human-like characteristics"], "all_classes": null, "length": 5136}
{"pred": " Reuters-8 dataset ", "answers": ["Reuters-8 dataset without stop words", "The Reuters-8 dataset (with stop words removed)"], "all_classes": null, "length": 5147}
{"pred": " The accuracy merits of the approach are demonstrated by improving LR by 5.17% (Accuracy) and 18.38% (AUC), and MLP by 10.71% (Accuracy) and 30.27% (AUC) on average. ", "answers": ["significant improvements clearly demonstrate that our approach is effective at improving model performance", "By evaluating the performance of the approach using accuracy and AUC"], "all_classes": null, "length": 4489}
{"pred": " The authors present evidence that the model can capture some biases in data annotation and collection by examining the test datasets and their confusion matrices, performing a manual inspection on a subset of the data, and considering the words and context of the tweets. ", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4119}
{"pred": " SimpleQuestions and WebQSP ", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4524}
{"pred": " The model is more reliable for spelling, word order, and grammatical errors. ", "answers": ["grammatical, spelling and word order errors", "spelling, word order and grammatical errors"], "all_classes": null, "length": 4579}
{"pred": " The authors crawled over 2M tweets from Twitter to build a dataset with 262,755 ironic and 112,330 non-ironic tweets. ", "answers": ["They developed a classifier to find ironic sentences in twitter data", "by crawling"], "all_classes": null, "length": 4599}
{"pred": " Global context refers to the whole document, while local context refers to the section/topic of the document. ", "answers": ["global (the whole document), local context (e.g., the section/topic)", "global (the whole document) and the local context (e.g., the section/topic) "], "all_classes": null, "length": 4287}
{"pred": " Knowledge Base Question Answering ", "answers": ["Knowledge Base Question Answering", "Knowledge Base Question Answering "], "all_classes": null, "length": 4521}
{"pred": " unanswerable ", "answers": ["the LDA approaches to recommendation systems and given the importance of research, we have studied recent impressive articles on this subject and presented a taxonomy of recommendation systems based on LDA of the recent research, we evaluated ISWC and WWW conferences articles from DBLP website and used the Gibbs sampling algorithm as an evaluation parameter", "discover the trends of the topics and find relationship between LDA topics and paper features and generate trust tags,  learn a LDA model with 100 topics; $\\alpha =0.01$, $\\beta = 0.01$ and using Gibbs sampling as a parameter estimation"], "all_classes": null, "length": 4322}
{"pred": " Yes ", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4127}
{"pred": " The question asks for the size of the imbalance in analyzed corpora. The article states that 65% of the speakers are men, speaking more than 75% of the time. This indicates a significant imbalance in gender representation within the corpora. Therefore, the answer is \"imbalance in analyzed corpora is significant\". ", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4055}
{"pred": " unanswerable ", "answers": ["by answering always YES (in batch 2 and 3) "], "all_classes": null, "length": 6817}
{"pred": " 0.7033 ", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 6810}
{"pred": " BLEU, NIST, METEOR, ROUGE-L, and CIDEr metrics were used to evaluate the generated text. ", "answers": ["BLEU , NIST , METEOR , ROUGE-L, CIDEr , evaluation script, automatic evaluation, human evaluation, minimum edit evaluation, word error rate (WER), factual errors and their types, fluency issues, acceptability of the output for production use in a news agency", "BLEU, NIST, METEOR, ROUGE-L, CIDEr"], "all_classes": null, "length": 4738}
{"pred": " unanswerable ", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4177}
{"pred": " CyberAttack and PoliticianDeath. ", "answers": ["Tweets related to CyberAttack and tweets related to PoliticianDeath", "cyber security (CyberAttack), death of politicians (PoliticianDeath)"], "all_classes": null, "length": 4481}
{"pred": " Logistic Regression and deep learning models. ", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 7257}
{"pred": " The strong baseline is the Conditional Copy (CC) model proposed by Wiseman. ", "answers": ["Conditional Copy (CC) model ", "delayed copy model (DEL),  template system (TEM), conditional copy (CC), NCP+CC (NCP)"], "all_classes": null, "length": 4746}
{"pred": " The invertibility condition is a constraint on the neural projector that requires it to be invertible, meaning that it must have a nonzero Jacobian determinant. This constraint enables tractable exact inference and marginal likelihood computation. ", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4323}
{"pred": " Logistic Regression (LR) and Multilayer Perceptron (MLP) are used as the classifiers. ", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4475}
{"pred": " unanswerable ", "answers": ["100 000 documents", " INLINEFORM1 cases"], "all_classes": null, "length": 4104}
{"pred": " The data was collected by first selecting 80 scenarios from three script data collections and creating 20 new scenarios. For each scenario, 20 texts were written by workers. Questions were then asked to be formed around these scenarios, with workers imagining they were explaining the scenario to a child. The questions were categorized as text-based or script-based, and answer candidates were provided by multiple participants for each question. ", "answers": ["The data was collected using 3 components: describe a series of pilot studies that were conducted to collect commonsense inference questions, then discuss the resulting data collection of questions, texts and answers via crowdsourcing on Amazon Mechanical Turk and gives information about some necessary postprocessing steps and the dataset validation."], "all_classes": null, "length": 4536}
{"pred": " The methods used to reduce data sparsity effects are back-translated texts and mix-source data. ", "answers": ["Back Translation, Mix-Source Approach", "data augmentation"], "all_classes": null, "length": 4123}
{"pred": " The difference in performance between the proposed model and baselines is significant, as the proposed model achieves a noticeable improvement in Hits@n/N accuracy and other metrics for retrieving the correct response of five diverse characters with different identities. ", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 5151}
{"pred": " unanswerable ", "answers": ["Frequent use of direct animal name calling, using simile and metaphors, through indirect speech like sarcasm, wishing evil to others, name alteration, societal stratification, immoral behavior and sexually related uses.", "Direct name calling, Simile and metaphor, Indirect speech, Wishing Evil, Name alteration, Societal stratification, Immoral behavior, Sexually related"], "all_classes": null, "length": 4566}
{"pred": " The authors show that their learned policy generalize better than existing solutions to unseen games by training a Seq2Seq model on the trajectories found by Go-Explore and showing its stronger performance on unseen games compared to existing competitive baselines. ", "answers": ["promising results by solving almost half of the unseen games, most of the lost games are in the hardest set, where a very long sequence of actions is required for winning the game"], "all_classes": null, "length": 4964}
{"pred": " The goal is to understand the impact of gender representation in data on gender bias in AI, specifically in the context of automatic speech recognition (ASR) systems trained on broadcast recordings. ", "answers": ["create fair systems", " broadcast recordings are also a valuable source of data for the speech processing community, recent works uncovering gender bias in several natural language processing (NLP) tools"], "all_classes": null, "length": 4063}
{"pred": " They build a predictive model to classify dogmatic posts from Reddit. ", "answers": ["logistic regression models", "logistic regression models based on unigram bag-of-words features (BOW), sentiment signals (SENT), the linguistic features from our earlier analyses (LING), and combinations of these features."], "all_classes": null, "length": 4208}
{"pred": " unanswerable ", "answers": ["No", "Unanswerable"], "all_classes": null, "length": 4181}
{"pred": " unanswerable ", "answers": ["doc2vec , CNN, DAN, Tree-LSTM, DRNN, LSTMN, C-LSTM, SPGK, WMD, S-WMD, Semantic-CNN, LSTM-GRNN, HN-ATT"], "all_classes": null, "length": 4169}
{"pred": " They measure which words are under-translated by NMT models by exploiting the word importance calculated by the Attribution method. ", "answers": ["They measured the under-translated words with low word importance score as calculated by Attribution.\nmethod", "we ask ten human annotators to manually label the under-translated input words, and at least two annotators label each input-hypothesis pair"], "all_classes": null, "length": 4246}
{"pred": " The model is applied to two datasets: the expanded version of the annotated Wikipedia conversations dataset and the subreddit ChangeMyView (CMV). ", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4718}
{"pred": " unanswerable ", "answers": ["No"], "all_classes": null, "length": 4117}
{"pred": " The dataset used is the Europarl corpus. ", "answers": ["Europarl corpus , WMT newstest 2014, News-Commentary-11, Wikipedia from WMT 2014, Multi-UN, EU-Bookshop, Rapid, Common-Crawl (WMT 2017)", "Europarl tests from 2006, 2007, 2008; WMT newstest 2014."], "all_classes": null, "length": 4184}
{"pred": " Yes ", "answers": ["No", "No"], "all_classes": null, "length": 4484}
{"pred": " A second order co-occurrence matrix is a matrix that represents the similarity between words in a corpus. ", "answers": ["frequencies of the other words which occur with both of them (i.e., second order co–occurrences)", "The matrix containing co-occurrences of the words which occur with the both words of every given pair of words."], "all_classes": null, "length": 4271}
{"pred": " Precision, recall, F1 and accuracy. ", "answers": ["precision, recall, F1 and accuracy", "precision, recall, F1 , accuracy "], "all_classes": null, "length": 4101}
{"pred": " unanswerable ", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4770}
{"pred": " unanswerable ", "answers": ["Annotations from experts are used if they have already been collected."], "all_classes": null, "length": 4377}
{"pred": " The size of their dataset is not mentioned in the article. ", "answers": ["13,939"], "all_classes": null, "length": 4489}
{"pred": " Yes ", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4592}
{"pred": " Yes ", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4457}
{"pred": " ROGUE, Recall, Precision, INLINEFORM0, INLINEFORM1, and INLINEFORM2. ", "answers": ["Quantitative evaluation methods using ROUGE, Recall, Precision and F1.", "standard ROGUE metric, Recall, Precision and INLINEFORM0 scores for ROGUE-1,  INLINEFORM2 scores for ROGUE-2 and ROGUE-L"], "all_classes": null, "length": 4395}
{"pred": " The baseline methods are: (1) Naive, (2) mSDA, (3) NaiveNN, (4) AuxNN, (5) ADAN, and (6) MMD. ", "answers": ["(1) Naive, (2) mSDA BIBREF7, (3) NaiveNN, (4) AuxNN BIBREF4, (5) ADAN BIBREF16, (6) MMD", "non-domain-adaptive baseline with bag-of-words representations and SVM classifier, mSDA, non-domain-adaptive CNN trained on source domain, neural model that exploits auxiliary tasks, adversarial training to reduce representation difference between domains, variants of deep CNNs are used for encoding images and the MMDs of multiple layers are jointly minimized"], "all_classes": null, "length": 5063}
{"pred": " unanswerable ", "answers": ["59.4% on wikipedia dataset, 93.4% on peer-reviewed archive AI papers, 77.1%  on peer-reviewed archive Computation and Language papers, and 79.9% on peer-reviewed archive Machine Learning papers"], "all_classes": null, "length": 4203}
{"pred": " The paper explores Skip-gram and CBOW embedding techniques. ", "answers": ["Skip–gram, CBOW", "integrated vector-res, vector-faith, Skip–gram, CBOW"], "all_classes": null, "length": 4259}
{"pred": " The two large-scale datasets used are the US dataset and the Italian dataset. ", "answers": ["US dataset, Italian dataset", "US dataset, Italian dataset"], "all_classes": null, "length": 4857}
{"pred": " unanswerable ", "answers": ["Influential leaders are more likely to change their profile attributes than their followers; the leaders do not change their usernames, while their followers change their usernames a lot; the leaders  tend to make new changes related to previous attribute values, while the followers make comparatively less related changes to previous attribute values."], "all_classes": null, "length": 5092}
{"pred": " They show there is space for further improvement by testing humans on a random subset of questions that the psr ensemble could not answer correctly and finding that a majority of these questions are answerable. ", "answers": [" by testing humans on a random subset of 50 named entity and 50 common noun validation questions that the psr ensemble could not answer correctly", "majority of questions that our system could not answer so far are in fact answerable"], "all_classes": null, "length": 4232}
{"pred": " The macro-averaged F-score is used as the metric for evaluation in this study. ", "answers": ["precision, recall , F1 score"], "all_classes": null, "length": 4515}
{"pred": " unanswerable ", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 7251}
{"pred": " The data set is not explicitly mentioned in the article, so the question is unanswerable. ", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": " unanswerable ", "answers": ["No", "No"], "all_classes": null, "length": 4071}
{"pred": " Avg. MCC and avg. +ve F1 score. ", "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 5869}
{"pred": " unanswerable ", "answers": ["Domain experts and fellow researchers can provide feedback on questions and help with dynamically revising them., connect to multiple disciplines, dual use", "Modeling considerations:  the variables (both predictors and outcomes)  are rarely simply binary or categorical;  using a particular classification scheme means deciding which variations are visible,; Supervised and unsupervised learning are the most common approaches to learning from data;  the unit of text that we are labeling (or annotating, or coding), either automatic or manual, can sometimes be different than one's final unit of analysis."], "all_classes": null, "length": 8530}
{"pred": " unanswerable ", "answers": ["Unanswerable"], "all_classes": null, "length": 8508}
{"pred": " unanswerable ", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 10349}
{"pred": " unanswerable ", "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": " SVMhmm ", "answers": ["Structural Support Vector Machine", "SVMhmm "], "all_classes": null, "length": 14468}
{"pred": " Yes ", "answers": ["Yes", "Yes"], "all_classes": null, "length": 12950}
{"pred": " unanswerable ", "answers": ["ELIZA,  PARRY, A.L.I.C.E., Cleverbot"], "all_classes": null, "length": 13395}
{"pred": " The dataset used is the UN General Debate Corpus, which contains every country statement in the UN General Debate between 1970 and 2014. ", "answers": ["corpus of state speeches delivered during the annual UN General Debate", "corpus of state speeches delivered during the annual UN General Debate"], "all_classes": null, "length": 8647}
{"pred": " Doc2Vec ", "answers": ["Document to Vector (Doc2Vec)", "Doc2Vec, PV-DBOW model"], "all_classes": null, "length": 8196}
{"pred": " Claim, Premise, Rebuttal, Refutation ", "answers": ["claim, premise, backing, rebuttal, and refutation", "claim, premise, backing, rebuttal, refutation"], "all_classes": null, "length": 14472}
{"pred": " unanswerable ", "answers": ["Yes", "Yes"], "all_classes": null, "length": 14465}
{"pred": " unanswerable ", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 13401}
{"pred": " unanswerable ", "answers": ["Yes", "Yes"], "all_classes": null, "length": 12952}
{"pred": " A node in the network approach represents a state. ", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 8641}
{"pred": " The data come from a variety of sources and no assumptions about its actual content with respect to argumentation can be drawn. ", "answers": ["user comments to newswire articles or to blog posts, forum posts, blog posts, newswire articles", "refer to each article, blog post, comment, or forum posts as a document"], "all_classes": null, "length": 14481}
{"pred": " They identify discussions of LGBTQ people in the New York Times by extracting paragraphs containing any word from a predetermined list of LGBTQ terms, and then applying various techniques to measure dehumanization. ", "answers": ["act paragraphs containing any word from a predetermined list of LGTBQ terms "], "all_classes": null, "length": 12970}
{"pred": " Yes ", "answers": ["No", "No"], "all_classes": null, "length": 8509}
{"pred": " Thorny issues not always at the forefront of discussions about computational text analysis methods. ", "answers": ["identifying the questions we wish to explore, Can text analysis provide a new perspective on a “big question” that has been attracting interest for years?, How can we explain what we observe?, hope to connect to multiple disciplines"], "all_classes": null, "length": 8555}
{"pred": " Time taken to answer utterances and resource consumption metrics (memory, CPU, network, disk). ", "answers": ["precision, recall, F1 and accuracy", "Response time, resource consumption (memory, CPU, network bandwidth), precision, recall, F1, accuracy."], "all_classes": null, "length": 13391}
{"pred": " unanswerable ", "answers": ["No", "Yes"], "all_classes": null, "length": 8643}
{"pred": " Different registers and domains pose challenges such as the need for generalization over several registers, the requirement for reliable annotations and sufficient data size, and the necessity to distinguish whether the texts are argumentative in unrestricted Web-based sources. ", "answers": ["linguistic variability"], "all_classes": null, "length": 14469}
{"pred": " The datasets were annotated by 145 human annotators who scored each pair of words on a scale of 0 to 6, indicating how semantically similar the two words in a given pair are. Annotators were required to be native speakers of the target language and were kept anonymous during the annotation process. The annotation process involved three rounds of scoring and re-evaluation to ensure consistency and agreement among annotators. ", "answers": ["1. Each annotator must assign an integer score between 0 and 6 (inclusive) indicating how semantically similar the two words in a given pair are. A score of 6 indicates very high similarity (i.e., perfect synonymy), while zero indicates no similarity., 2. Each annotator must score the entire set of 1,888 pairs in the dataset.,  able to use external sources (e.g. dictionaries, thesauri, WordNet) if required, not able to communicate with each other during the annotation process"], "all_classes": null, "length": 14722}
{"pred": " unanswerable ", "answers": ["Unanswerable"], "all_classes": null, "length": 14471}
{"pred": " unanswerable ", "answers": ["Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese", "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese"], "all_classes": null, "length": 14660}
